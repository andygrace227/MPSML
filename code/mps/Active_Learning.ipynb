{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd378db6-e186-4c42-9b26-cc0f91523042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as f\n",
    "import numpy as np \n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt \n",
    "import auto_encoder\n",
    "import math\n",
    "import bisect\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ecfdd27-8776-49e6-aabb-0b6b5d56ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(num_samples):\n",
    "    pts = []\n",
    "    i = 0\n",
    "    it1 = int(math.sqrt(num_samples)/10)\n",
    "    it2 = int((num_samples/10) - math.sqrt(num_samples))\n",
    "    for j in range (10):\n",
    "        for k in range(10):\n",
    "            pts.append(i)\n",
    "            i+=it1\n",
    "        i+=it2\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d026dfa5-1a8d-440d-bc4a-d34854fe5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_magnetization(arr_seq, num_qubits):\n",
    "    \n",
    "    mag_vec = []\n",
    "    for elem in arr_seq:\n",
    "        magnetization = 0\n",
    "        for char in elem:\n",
    "            temp = (int(char)*-2)+1\n",
    "            magnetization += temp \n",
    "        mag_vec.append(magnetization)\n",
    "    mag_vec = np.array(mag_vec)\n",
    "    mag_vec = mag_vec / (num_qubits)\n",
    "    return mag_vec   \n",
    "\n",
    "def seq_gen(num_q):\n",
    "    if num_q == 2:\n",
    "        return ['00','01', '10','11']\n",
    "    else:\n",
    "        temp = []\n",
    "        \n",
    "        smaller_vals = seq_gen(num_q-1)\n",
    "        for i in ['0','1']:\n",
    "            for each in smaller_vals:\n",
    "                temp.append(i+each)\n",
    "        return temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ddfa5fa-24e2-4a68-87ed-9c3373a35d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_points(mag, mag_t, num):\n",
    "    diffs = abs((mag+1) - (mag_t+1))/(mag_t+1)\n",
    "    points = (-np.asarray(diffs)).argsort()[:num]\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4e7135d-6c69-4345-b91c-9cf5ab3ee180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_points(data1, qubits, error_points):    \n",
    "    data = np.load(data1)\n",
    "    Bx = data['fields'].T[qubits]\n",
    "    Bz = data['fields'].T[2*qubits]\n",
    "    points = []\n",
    "    for i in error_points:\n",
    "        points.append((Bx[i],Bz[i]))\n",
    "    print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c2c356a-dd12-4838-8d36-348821fd532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_pool(old_pool, error_points, max_pool_size):\n",
    "    for i in error_points:\n",
    "        num = int(i)\n",
    "        if(i not in old_pool):\n",
    "            bisect.insort(old_pool,i)\n",
    "        else:\n",
    "            Bz_low_num = num-1\n",
    "            Bz_high_num= num+1\n",
    "            Bz_low_flag=True\n",
    "            Bz_high_flag=True\n",
    "            \n",
    "            Bx_low_num = int(num-math.sqrt(max_pool_size))\n",
    "            Bx_high_num= int(num+math.sqrt(max_pool_size))\n",
    "            Bx_low_flag=True\n",
    "            Bx_high_flag=True\n",
    "            \n",
    "            while(Bz_low_flag or Bz_high_flag or Bx_low_flag or Bx_high_flag):\n",
    "                if(Bz_low_flag):\n",
    "                    if(Bz_low_num<=0 or Bz_low_num % math.sqrt(max_pool_size)==math.sqrt(max_pool_size)-1):\n",
    "                        Bz_low_flag=False\n",
    "                    elif(Bz_low_num not in old_pool):\n",
    "                        bisect.insort(old_pool,Bz_low_num)\n",
    "                        Bz_low_flag=False\n",
    "                    else:\n",
    "                        Bz_low_num-=1\n",
    "                if(Bz_high_flag):\n",
    "                    if(Bz_high_num>=max_pool_size or Bz_high_num % math.sqrt(max_pool_size)==0):\n",
    "                        Bz_high_flag=False\n",
    "                    elif(Bz_high_num not in old_pool):\n",
    "                        bisect.insort(old_pool, Bz_high_num)\n",
    "                        Bz_high_flag=False\n",
    "                    else:\n",
    "                        Bz_high_num+=1\n",
    "                if(Bx_low_flag):\n",
    "                    if(Bx_low_num<=0):\n",
    "                        Bx_low_flag=False\n",
    "                    elif(Bx_low_num not in old_pool):\n",
    "                        bisect.insort(old_pool, Bx_low_num)\n",
    "                        Bx_low_flag=False\n",
    "                    else:\n",
    "                        Bx_low_num-=int(math.sqrt(max_pool_size))\n",
    "                if(Bx_high_flag):\n",
    "                    if(Bx_high_num>=max_pool_size or Bx_high_num % math.sqrt(max_pool_size)==0):\n",
    "                        Bx_high_flag=False\n",
    "                    elif(Bx_high_num not in old_pool):\n",
    "                        bisect.insort(old_pool, Bx_high_num)\n",
    "                        Bx_high_flag=False\n",
    "                    else:\n",
    "                        Bx_high_num+=int(math.sqrt(max_pool_size))    \n",
    "                    \n",
    "    return old_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6ea3ad3-eb53-402f-b91b-f5978bb839dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset = auto_encoder.get_dataset\n",
    "data_2 = '2_qubit_crit_data.npz'\n",
    "data_4 = '4_qubit_crit_data.npz'\n",
    "data_6 = '6_qubit_crit_data.npz'\n",
    "data_7 = '7_qubit_crit_data.npz'\n",
    "data_8 = '8_qubit_crit_data.npz'\n",
    "data_9 = '9_qubit_crit_data.npz'\n",
    "\n",
    "training_n_sizes = [2,4,7]\n",
    "validation_n_sizes = [6,8,9]\n",
    "\n",
    "pts = seed(1600)\n",
    "\n",
    "final_warmup_loss = [[0]*3 for i in range(5)]\n",
    "final_training_epoch_loss = [[0]*3 for i in range(5)]\n",
    "final_training_loss = [[0]*3 for i in range(5)]\n",
    "final_validation_loss = [[0]*6 for i in range(5)]\n",
    "\n",
    "magnetization_6 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdf54683-642a-48ef-babb-ca89e25f693a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARMUP TRAINING ITERATION:  0\n",
      "2 \t 349.60400390625\n",
      "4 \t 221.23236083984375\n",
      "7 \t 10474.4248046875\n",
      "2 \t 15.572617530822754\n",
      "4 \t 57.455284118652344\n",
      "7 \t 817.985107421875\n",
      "2 \t 12.251035690307617\n",
      "4 \t 47.5793342590332\n",
      "7 \t 735.078125\n",
      "2 \t 7.251011848449707\n",
      "4 \t 44.307655334472656\n",
      "7 \t 568.8013916015625\n",
      "2 \t 6.958998680114746\n",
      "4 \t 37.92871856689453\n",
      "7 \t 655.7412109375\n",
      "2 \t 7.0430378913879395\n",
      "4 \t 34.55327606201172\n",
      "7 \t 546.4805908203125\n",
      "2 \t 8.252176284790039\n",
      "4 \t 38.418827056884766\n",
      "7 \t 504.28631591796875\n",
      "2 \t 6.020088195800781\n",
      "4 \t 32.487796783447266\n",
      "7 \t 523.510009765625\n",
      "2 \t 4.779666900634766\n",
      "4 \t 30.950592041015625\n",
      "7 \t 503.713134765625\n",
      "2 \t 5.717419624328613\n",
      "4 \t 29.40480613708496\n",
      "7 \t 439.81695556640625\n",
      "__________________________________________________\n",
      "\n",
      "TRAINING ITERATION:  0\n",
      "Cycle: \t 0\n",
      "****************************************\n",
      "2 \t 4.375389099121094\n",
      "2 \t 2.4086737632751465\n",
      "2 \t 1.5314345359802246\n",
      "4 \t 39.83738327026367\n",
      "4 \t 17.32412338256836\n",
      "4 \t 16.151567459106445\n",
      "7 \t 210.1952362060547\n",
      "7 \t 189.37107849121094\n",
      "7 \t 119.90451049804688\n",
      "Cycle: \t 1\n",
      "****************************************\n",
      "2 \t 3.003312110900879\n",
      "2 \t 0.6121448874473572\n",
      "2 \t 0.6677169799804688\n",
      "4 \t 10.885394096374512\n",
      "4 \t 14.800943374633789\n",
      "4 \t 8.844776153564453\n",
      "7 \t 96.22522735595703\n",
      "7 \t 66.20059967041016\n",
      "7 \t 72.40858459472656\n",
      "Cycle: \t 2\n",
      "****************************************\n",
      "2 \t 1.3477191925048828\n",
      "2 \t 0.5978196859359741\n",
      "2 \t 0.3242260813713074\n",
      "4 \t 12.277292251586914\n",
      "4 \t 3.6911985874176025\n",
      "4 \t 5.980794906616211\n",
      "7 \t 111.21484375\n",
      "7 \t 55.19500732421875\n",
      "7 \t 22.907676696777344\n",
      "Cycle: \t 3\n",
      "****************************************\n",
      "2 \t 0.8851946592330933\n",
      "2 \t 0.6656786203384399\n",
      "2 \t 0.45440560579299927\n",
      "4 \t 66.44635009765625\n",
      "4 \t 3.3584399223327637\n",
      "4 \t 3.145124912261963\n",
      "7 \t 76.20345306396484\n",
      "7 \t 46.69676971435547\n",
      "7 \t 27.12558937072754\n",
      "Cycle: \t 4\n",
      "****************************************\n",
      "2 \t 0.7439276576042175\n",
      "2 \t 0.3924598693847656\n",
      "2 \t 0.22534358501434326\n",
      "4 \t 10.101818084716797\n",
      "4 \t 3.8611578941345215\n",
      "4 \t 2.0738420486450195\n",
      "7 \t 70.41731262207031\n",
      "7 \t 30.611804962158203\n",
      "7 \t 14.698702812194824\n",
      "Cycle: \t 5\n",
      "****************************************\n",
      "2 \t 0.4333328306674957\n",
      "2 \t 0.33396071195602417\n",
      "2 \t 0.4838535189628601\n",
      "4 \t 6.6881513595581055\n",
      "4 \t 1.402897596359253\n",
      "4 \t 1.768819808959961\n",
      "7 \t 52.930511474609375\n",
      "7 \t 18.560165405273438\n",
      "7 \t 23.61591339111328\n",
      "Cycle: \t 6\n",
      "****************************************\n",
      "2 \t 0.678093433380127\n",
      "2 \t 0.26140162348747253\n",
      "2 \t 1.0494226217269897\n",
      "4 \t 4.124117851257324\n",
      "4 \t 11.995075225830078\n",
      "4 \t 8.676921844482422\n",
      "7 \t 101.44146728515625\n",
      "7 \t 15.450772285461426\n",
      "7 \t 32.6431884765625\n",
      "Cycle: \t 7\n",
      "****************************************\n",
      "2 \t 0.6927375197410583\n",
      "2 \t 0.2863486707210541\n",
      "2 \t 0.19742617011070251\n",
      "4 \t 2.380830764770508\n",
      "4 \t 0.8284726142883301\n",
      "4 \t 1.405029296875\n",
      "7 \t 118.45216369628906\n",
      "7 \t 115.91268920898438\n",
      "7 \t 9.441508293151855\n",
      "Cycle: \t 8\n",
      "****************************************\n",
      "2 \t 0.4656699299812317\n",
      "2 \t 0.1441662311553955\n",
      "2 \t 0.156015545129776\n",
      "4 \t 2.3608813285827637\n",
      "4 \t 6.338684558868408\n",
      "4 \t 1.7255887985229492\n",
      "7 \t 51.261260986328125\n",
      "7 \t 8.636578559875488\n",
      "7 \t 49.34605407714844\n",
      "Cycle: \t 9\n",
      "****************************************\n",
      "2 \t 0.40430545806884766\n",
      "2 \t 0.18552935123443604\n",
      "2 \t 0.10216493904590607\n",
      "4 \t 9.537196159362793\n",
      "4 \t 1.5965871810913086\n",
      "4 \t 9.85230827331543\n",
      "7 \t 30.636634826660156\n",
      "7 \t 17.278472900390625\n",
      "7 \t 7.183125972747803\n",
      "__________________________________________________\n",
      "\n",
      "TRAINING LOSS ITERATION:  0\n",
      "2 \t 0.3622112274169922\n",
      "4 \t 2.3113183975219727\n",
      "7 \t 32.28742599487305\n",
      "2 \t 0.23759695887565613\n",
      "4 \t 2.7156693935394287\n",
      "7 \t 27.92157745361328\n",
      "2 \t 0.21961180865764618\n",
      "4 \t 2.5907552242279053\n",
      "7 \t 14.65159797668457\n",
      "2 \t 0.49305444955825806\n",
      "4 \t 3.557223081588745\n",
      "7 \t 28.500825881958008\n",
      "2 \t 0.4188414216041565\n",
      "4 \t 1.477677822113037\n",
      "7 \t 18.578771591186523\n",
      "2 \t 0.5130534172058105\n",
      "4 \t 2.4654345512390137\n",
      "7 \t 27.539592742919922\n",
      "2 \t 0.16111762821674347\n",
      "4 \t 2.0464682579040527\n",
      "7 \t 25.257240295410156\n",
      "2 \t 0.3448525071144104\n",
      "4 \t 2.2112791538238525\n",
      "7 \t 22.93161392211914\n",
      "2 \t 0.3900047540664673\n",
      "4 \t 1.2371907234191895\n",
      "7 \t 17.023801803588867\n",
      "2 \t 0.23229581117630005\n",
      "4 \t 1.306187629699707\n",
      "7 \t 27.36439323425293\n",
      "__________________________________________________\n",
      "\n",
      "VALIDATION LOSS ITERATION:  0\n",
      "6 \t 0.0011800444917753339\n",
      "8 \t 0.0014640765730291605\n",
      "9 \t 0.0021213339641690254\n",
      "2 \t 0.0006914634141139686\n",
      "4 \t 0.006026024464517832\n",
      "7 \t 0.009879915043711662\n",
      "__________________________________________________\n",
      "\n",
      "ERROR POINTS:\n",
      "NEW SET: \n",
      "[0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 80, 120, 160, 161, 164, 168, 172, 176, 180, 184, 188, 192, 196, 200, 240, 280, 320, 324, 328, 332, 336, 340, 344, 348, 352, 356, 441, 480, 481, 484, 488, 492, 496, 500, 504, 508, 512, 516, 521, 640, 644, 648, 652, 656, 660, 664, 668, 672, 676, 800, 804, 808, 812, 816, 820, 824, 828, 832, 836, 960, 964, 968, 972, 976, 980, 984, 988, 992, 996, 1120, 1124, 1128, 1132, 1136, 1140, 1144, 1148, 1152, 1156, 1280, 1284, 1288, 1292, 1296, 1300, 1304, 1308, 1312, 1316, 1440, 1444, 1448, 1452, 1456, 1460, 1464, 1468, 1472, 1476]\n",
      "\n",
      "_________________________________\n",
      "WARMUP TRAINING ITERATION:  1\n",
      "2 \t 185.93975830078125\n",
      "4 \t 388.0690612792969\n",
      "7 \t 5827.69970703125\n",
      "2 \t 15.697728157043457\n",
      "4 \t 69.4568099975586\n",
      "7 \t 835.401611328125\n",
      "2 \t 7.883301734924316\n",
      "4 \t 54.843116760253906\n",
      "7 \t 894.9825439453125\n",
      "2 \t 7.16988468170166\n",
      "4 \t 43.65120315551758\n",
      "7 \t 629.7564697265625\n",
      "2 \t 6.065733909606934\n",
      "4 \t 38.592498779296875\n",
      "7 \t 625.210205078125\n",
      "2 \t 5.57913875579834\n",
      "4 \t 44.36949920654297\n",
      "7 \t 547.2802124023438\n",
      "2 \t 5.388783931732178\n",
      "4 \t 39.810089111328125\n",
      "7 \t 538.6834716796875\n",
      "2 \t 5.352200984954834\n",
      "4 \t 33.83524703979492\n",
      "7 \t 709.03076171875\n",
      "2 \t 6.252837181091309\n",
      "4 \t 34.3407096862793\n",
      "7 \t 630.277099609375\n",
      "2 \t 5.296435356140137\n",
      "4 \t 30.95516014099121\n",
      "7 \t 601.9066162109375\n",
      "__________________________________________________\n",
      "\n",
      "TRAINING ITERATION:  1\n",
      "Cycle: \t 0\n",
      "****************************************\n",
      "2 \t 4.96922492980957\n",
      "2 \t 2.8797764778137207\n",
      "2 \t 3.061887741088867\n",
      "4 \t 38.4180908203125\n",
      "4 \t 24.478050231933594\n",
      "4 \t 20.41968536376953\n",
      "7 \t 649.9823608398438\n",
      "7 \t 298.5561218261719\n",
      "7 \t 184.52703857421875\n",
      "Cycle: \t 1\n",
      "****************************************\n",
      "2 \t 6.6133713722229\n",
      "2 \t 2.2310068607330322\n",
      "2 \t 1.4028258323669434\n",
      "4 \t 31.362316131591797\n",
      "4 \t 9.202224731445312\n",
      "4 \t 15.837345123291016\n",
      "7 \t 602.973876953125\n",
      "7 \t 305.962890625\n",
      "7 \t 125.42039489746094\n",
      "Cycle: \t 2\n",
      "****************************************\n",
      "2 \t 5.580493927001953\n",
      "2 \t 1.0515367984771729\n",
      "2 \t 0.8316419124603271\n",
      "4 \t 26.257251739501953\n",
      "4 \t 9.225568771362305\n",
      "4 \t 3.9947118759155273\n",
      "7 \t 664.7088623046875\n",
      "7 \t 348.1961669921875\n",
      "7 \t 166.69400024414062\n",
      "Cycle: \t 3\n",
      "****************************************\n",
      "2 \t 5.871410846710205\n",
      "2 \t 1.5123481750488281\n",
      "2 \t 0.9545531272888184\n",
      "4 \t 19.206281661987305\n",
      "4 \t 14.242387771606445\n",
      "4 \t 7.312836647033691\n",
      "7 \t 442.3290100097656\n",
      "7 \t 128.4031524658203\n",
      "7 \t 143.34323120117188\n",
      "Cycle: \t 4\n",
      "****************************************\n",
      "2 \t 4.799471378326416\n",
      "2 \t 1.1760081052780151\n",
      "2 \t 0.7698278427124023\n",
      "4 \t 12.581704139709473\n",
      "4 \t 18.20208740234375\n",
      "4 \t 9.48536491394043\n",
      "7 \t 145.92124938964844\n",
      "7 \t 140.9373016357422\n",
      "7 \t 93.68429565429688\n",
      "Cycle: \t 5\n",
      "****************************************\n",
      "2 \t 1.410635232925415\n",
      "2 \t 0.37429559230804443\n",
      "2 \t 0.4275241196155548\n",
      "4 \t 11.448553085327148\n",
      "4 \t 2.4140872955322266\n",
      "4 \t 6.908255577087402\n",
      "7 \t 127.88958740234375\n",
      "7 \t 152.82711791992188\n",
      "7 \t 235.364501953125\n",
      "Cycle: \t 6\n",
      "****************************************\n",
      "2 \t 1.691467046737671\n",
      "2 \t 0.9310213327407837\n",
      "2 \t 0.7364844679832458\n",
      "4 \t 16.19673728942871\n",
      "4 \t 5.983189582824707\n",
      "4 \t 1.4127693176269531\n",
      "7 \t 126.45453643798828\n",
      "7 \t 109.00736999511719\n",
      "7 \t 96.73107147216797\n",
      "Cycle: \t 7\n",
      "****************************************\n",
      "2 \t 0.9065907001495361\n",
      "2 \t 0.5639471411705017\n",
      "2 \t 0.5041614770889282\n",
      "4 \t 11.679269790649414\n",
      "4 \t 2.7385268211364746\n",
      "4 \t 0.7558310031890869\n",
      "7 \t 130.74606323242188\n",
      "7 \t 216.8221435546875\n",
      "7 \t 90.84056091308594\n",
      "Cycle: \t 8\n",
      "****************************************\n",
      "2 \t 0.4843190908432007\n",
      "2 \t 0.38056665658950806\n",
      "2 \t 0.3063620924949646\n",
      "4 \t 13.017477035522461\n",
      "4 \t 1.0144708156585693\n",
      "4 \t 1.154682993888855\n",
      "7 \t 117.88214111328125\n",
      "7 \t 80.77450561523438\n",
      "7 \t 83.12112426757812\n",
      "Cycle: \t 9\n",
      "****************************************\n",
      "2 \t 1.2851777076721191\n",
      "2 \t 0.4678952097892761\n",
      "2 \t 0.2483154833316803\n",
      "4 \t 4.820609092712402\n",
      "4 \t 1.0783467292785645\n",
      "4 \t 2.2272751331329346\n",
      "7 \t 111.98649597167969\n",
      "7 \t 91.61729431152344\n",
      "7 \t 74.34280395507812\n",
      "__________________________________________________\n",
      "\n",
      "TRAINING LOSS ITERATION:  1\n",
      "2 \t 0.943797767162323\n",
      "4 \t 15.286109924316406\n",
      "7 \t 137.12289428710938\n",
      "2 \t 0.3352605402469635\n",
      "4 \t 7.679948806762695\n",
      "7 \t 135.7192840576172\n",
      "2 \t 0.778479814529419\n",
      "4 \t 14.433773040771484\n",
      "7 \t 251.56121826171875\n",
      "2 \t 0.734420657157898\n",
      "4 \t 11.848467826843262\n",
      "7 \t 120.75889587402344\n",
      "2 \t 0.4644637703895569\n",
      "4 \t 12.323100090026855\n",
      "7 \t 102.84320068359375\n",
      "2 \t 0.8040265440940857\n",
      "4 \t 6.746021747589111\n",
      "7 \t 77.05155181884766\n",
      "2 \t 0.570448100566864\n",
      "4 \t 4.09465217590332\n",
      "7 \t 116.12421417236328\n",
      "2 \t 0.570368766784668\n",
      "4 \t 3.3250792026519775\n",
      "7 \t 81.93096160888672\n",
      "2 \t 0.42330384254455566\n",
      "4 \t 3.230719804763794\n",
      "7 \t 86.2732925415039\n",
      "2 \t 0.3679203987121582\n",
      "4 \t 4.875025749206543\n",
      "7 \t 88.27371978759766\n",
      "__________________________________________________\n",
      "\n",
      "VALIDATION LOSS ITERATION:  1\n",
      "6 \t 0.05678568407893181\n",
      "8 \t 0.03478614613413811\n",
      "9 \t 0.036460936069488525\n",
      "2 \t 0.05670595169067383\n",
      "4 \t 0.03458091616630554\n",
      "7 \t 0.04718600958585739\n",
      "__________________________________________________\n",
      "\n",
      "ERROR POINTS:\n",
      "NEW SET: \n",
      "[0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 80, 81, 120, 121, 160, 161, 162, 164, 168, 172, 176, 180, 184, 188, 192, 196, 200, 201, 240, 241, 280, 281, 320, 321, 324, 328, 332, 336, 340, 344, 348, 352, 356, 360, 400, 440, 441, 480, 481, 484, 488, 492, 496, 500, 504, 508, 512, 516, 521, 640, 644, 648, 652, 656, 660, 664, 668, 672, 676, 800, 804, 808, 812, 816, 820, 824, 828, 832, 836, 960, 964, 968, 972, 976, 980, 984, 988, 992, 996, 1120, 1124, 1128, 1132, 1136, 1140, 1144, 1148, 1152, 1156, 1280, 1284, 1288, 1292, 1296, 1300, 1304, 1308, 1312, 1316, 1440, 1444, 1448, 1452, 1456, 1460, 1464, 1468, 1472, 1476]\n",
      "\n",
      "_________________________________\n",
      "WARMUP TRAINING ITERATION:  2\n",
      "2 \t 45.14259338378906\n",
      "4 \t 232.29183959960938\n",
      "7 \t 2253.7236328125\n",
      "2 \t 11.493242263793945\n",
      "4 \t 84.05726623535156\n",
      "7 \t 651.271240234375\n",
      "2 \t 11.487447738647461\n",
      "4 \t 94.97915649414062\n",
      "7 \t 624.23291015625\n",
      "2 \t 9.42464542388916\n",
      "4 \t 75.14002227783203\n",
      "7 \t 449.0157470703125\n",
      "2 \t 9.089509963989258\n",
      "4 \t 73.31098937988281\n",
      "7 \t 372.9674987792969\n",
      "2 \t 6.198441505432129\n",
      "4 \t 63.280555725097656\n",
      "7 \t 389.21295166015625\n",
      "2 \t 7.436130523681641\n",
      "4 \t 59.68864440917969\n",
      "7 \t 333.97161865234375\n",
      "2 \t 4.893076419830322\n",
      "4 \t 60.92447280883789\n",
      "7 \t 389.3739013671875\n",
      "2 \t 3.951406955718994\n",
      "4 \t 61.44599151611328\n",
      "7 \t 281.66864013671875\n",
      "2 \t 4.756293296813965\n",
      "4 \t 51.92731475830078\n",
      "7 \t 385.39276123046875\n",
      "__________________________________________________\n",
      "\n",
      "TRAINING ITERATION:  2\n",
      "Cycle: \t 0\n",
      "****************************************\n",
      "2 \t 2.9771041870117188\n",
      "2 \t 3.894542694091797\n",
      "2 \t 1.5698606967926025\n",
      "4 \t 53.347877502441406\n",
      "4 \t 17.94054412841797\n",
      "4 \t 17.369384765625\n",
      "7 \t 231.544677734375\n",
      "7 \t 222.70040893554688\n",
      "7 \t 109.31864929199219\n",
      "Cycle: \t 1\n",
      "****************************************\n",
      "2 \t 3.8312182426452637\n",
      "2 \t 1.2133042812347412\n",
      "2 \t 1.117473840713501\n",
      "4 \t 30.31987953186035\n",
      "4 \t 17.386306762695312\n",
      "4 \t 10.179357528686523\n",
      "7 \t 215.2867431640625\n",
      "7 \t 133.8416748046875\n",
      "7 \t 89.28620910644531\n",
      "Cycle: \t 2\n",
      "****************************************\n",
      "2 \t 2.951460838317871\n",
      "2 \t 0.6522648334503174\n",
      "2 \t 0.2928042411804199\n",
      "4 \t 11.894638061523438\n",
      "4 \t 9.78439998626709\n",
      "4 \t 9.35421371459961\n",
      "7 \t 226.0070343017578\n",
      "7 \t 78.48934173583984\n",
      "7 \t 122.88087463378906\n",
      "Cycle: \t 3\n",
      "****************************************\n",
      "2 \t 2.642951488494873\n",
      "2 \t 0.29694250226020813\n",
      "2 \t 0.3241865634918213\n",
      "4 \t 10.760693550109863\n",
      "4 \t 3.3190927505493164\n",
      "4 \t 4.336828708648682\n",
      "7 \t 81.21781921386719\n",
      "7 \t 76.66009521484375\n",
      "7 \t 88.70578002929688\n",
      "Cycle: \t 4\n",
      "****************************************\n",
      "2 \t 2.3907577991485596\n",
      "2 \t 0.34852689504623413\n",
      "2 \t 0.3242747485637665\n",
      "4 \t 29.96009635925293\n",
      "4 \t 2.2234420776367188\n",
      "4 \t 1.5198014974594116\n",
      "7 \t 111.6827621459961\n",
      "7 \t 24.710649490356445\n",
      "7 \t 28.565956115722656\n",
      "Cycle: \t 5\n",
      "****************************************\n",
      "2 \t 1.6474970579147339\n",
      "2 \t 0.2424558848142624\n",
      "2 \t 0.541118323802948\n",
      "4 \t 12.094131469726562\n",
      "4 \t 1.4777005910873413\n",
      "4 \t 0.9635699987411499\n",
      "7 \t 68.72494506835938\n",
      "7 \t 49.39581298828125\n",
      "7 \t 36.89516067504883\n",
      "Cycle: \t 6\n",
      "****************************************\n",
      "2 \t 1.0326348543167114\n",
      "2 \t 0.5399868488311768\n",
      "2 \t 0.7307162880897522\n",
      "4 \t 6.0562238693237305\n",
      "4 \t 1.4641871452331543\n",
      "4 \t 2.004354238510132\n",
      "7 \t 31.777729034423828\n",
      "7 \t 32.80833435058594\n",
      "7 \t 85.36207580566406\n",
      "Cycle: \t 7\n",
      "****************************************\n",
      "2 \t 0.720288872718811\n",
      "2 \t 0.19179485738277435\n",
      "2 \t 0.28097003698349\n",
      "4 \t 12.800846099853516\n",
      "4 \t 2.053544521331787\n",
      "4 \t 1.8531376123428345\n",
      "7 \t 87.36683654785156\n",
      "7 \t 27.595300674438477\n",
      "7 \t 19.42547607421875\n",
      "Cycle: \t 8\n",
      "****************************************\n",
      "2 \t 0.5193097591400146\n",
      "2 \t 0.6108207702636719\n",
      "2 \t 0.16262555122375488\n",
      "4 \t 7.034722805023193\n",
      "4 \t 2.091118812561035\n",
      "4 \t 0.7748856544494629\n",
      "7 \t 22.16465950012207\n",
      "7 \t 30.911640167236328\n",
      "7 \t 60.82652282714844\n",
      "Cycle: \t 9\n",
      "****************************************\n",
      "2 \t 0.3335215151309967\n",
      "2 \t 0.20432370901107788\n",
      "2 \t 0.17460453510284424\n",
      "4 \t 2.9783527851104736\n",
      "4 \t 1.0349794626235962\n",
      "4 \t 1.2546086311340332\n",
      "7 \t 25.64657974243164\n",
      "7 \t 30.103483200073242\n",
      "7 \t 13.207186698913574\n",
      "__________________________________________________\n",
      "\n",
      "TRAINING LOSS ITERATION:  2\n",
      "2 \t 0.9295552968978882\n",
      "4 \t 1.382760763168335\n",
      "7 \t 38.345947265625\n",
      "2 \t 0.40904611349105835\n",
      "4 \t 2.1328125\n",
      "7 \t 40.98925018310547\n",
      "2 \t 0.9440068602561951\n",
      "4 \t 6.202322006225586\n",
      "7 \t 18.94618034362793\n",
      "2 \t 1.2568767070770264\n",
      "4 \t 3.913832187652588\n",
      "7 \t 64.72039031982422\n",
      "2 \t 0.5745616555213928\n",
      "4 \t 11.317157745361328\n",
      "7 \t 28.965023040771484\n",
      "2 \t 0.5860854387283325\n",
      "4 \t 3.859677791595459\n",
      "7 \t 180.39317321777344\n",
      "2 \t 1.5789847373962402\n",
      "4 \t 17.8862361907959\n",
      "7 \t 68.48286437988281\n",
      "2 \t 0.6251375079154968\n",
      "4 \t 1.4180140495300293\n",
      "7 \t 58.00916290283203\n",
      "2 \t 0.47409260272979736\n",
      "4 \t 3.363353729248047\n",
      "7 \t 44.39711380004883\n",
      "2 \t 0.4751667380332947\n",
      "4 \t 4.589102745056152\n",
      "7 \t 41.89576721191406\n",
      "__________________________________________________\n",
      "\n",
      "VALIDATION LOSS ITERATION:  2\n",
      "6 \t 0.06508499383926392\n",
      "8 \t 0.06555287539958954\n",
      "9 \t 0.07024335861206055\n",
      "2 \t 0.002687345491722226\n",
      "4 \t 0.006066800560802221\n",
      "7 \t 0.007501313928514719\n",
      "__________________________________________________\n",
      "\n",
      "ERROR POINTS:\n",
      "NEW SET: \n",
      "[0, 1, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 80, 81, 120, 121, 160, 161, 162, 164, 168, 172, 176, 180, 184, 188, 192, 196, 200, 201, 240, 241, 280, 281, 320, 321, 324, 328, 332, 336, 340, 344, 348, 352, 356, 360, 400, 440, 441, 480, 481, 484, 488, 492, 496, 500, 504, 508, 512, 516, 521, 640, 644, 648, 652, 656, 660, 664, 668, 672, 676, 681, 721, 761, 800, 801, 804, 808, 812, 816, 820, 824, 828, 832, 836, 841, 881, 921, 960, 961, 962, 964, 968, 972, 976, 980, 984, 988, 992, 996, 1120, 1124, 1128, 1132, 1136, 1140, 1144, 1148, 1152, 1156, 1280, 1284, 1288, 1292, 1296, 1300, 1304, 1308, 1312, 1316, 1440, 1444, 1448, 1452, 1456, 1460, 1464, 1468, 1472, 1476]\n",
      "\n",
      "_________________________________\n",
      "WARMUP TRAINING ITERATION:  3\n",
      "2 \t 45.1163215637207\n",
      "4 \t 203.495361328125\n",
      "7 \t 15203.130859375\n",
      "2 \t 29.493955612182617\n",
      "4 \t 921.9464111328125\n",
      "7 \t 1950.512451171875\n",
      "2 \t 15.86591625213623\n",
      "4 \t 59.049095153808594\n",
      "7 \t 1155.083740234375\n",
      "2 \t 11.498172760009766\n",
      "4 \t 47.40166473388672\n",
      "7 \t 812.8783569335938\n",
      "2 \t 9.020963668823242\n",
      "4 \t 41.658050537109375\n",
      "7 \t 921.874267578125\n",
      "2 \t 8.57651138305664\n",
      "4 \t 34.450469970703125\n",
      "7 \t 1424.550048828125\n",
      "2 \t 9.04344654083252\n",
      "4 \t 38.01826477050781\n",
      "7 \t 703.5914306640625\n",
      "2 \t 4.559892654418945\n",
      "4 \t 31.647384643554688\n",
      "7 \t 606.338134765625\n",
      "2 \t 5.130029678344727\n",
      "4 \t 35.10148620605469\n",
      "7 \t 781.3348388671875\n",
      "2 \t 6.357902526855469\n",
      "4 \t 29.607013702392578\n",
      "7 \t 753.9805297851562\n",
      "__________________________________________________\n",
      "\n",
      "TRAINING ITERATION:  3\n",
      "Cycle: \t 0\n",
      "****************************************\n",
      "2 \t 2.988555431365967\n",
      "2 \t 1.7192988395690918\n",
      "2 \t 1.80885910987854\n",
      "4 \t 33.539756774902344\n",
      "4 \t 21.039813995361328\n",
      "4 \t 21.394142150878906\n",
      "7 \t 667.2491455078125\n",
      "7 \t 317.98223876953125\n",
      "7 \t 311.09674072265625\n",
      "Cycle: \t 1\n",
      "****************************************\n",
      "2 \t 6.322403430938721\n",
      "2 \t 0.6984789371490479\n",
      "2 \t 0.346548855304718\n",
      "4 \t 20.685855865478516\n",
      "4 \t 12.320394515991211\n",
      "4 \t 11.407943725585938\n",
      "7 \t 277.1329345703125\n",
      "7 \t 246.52877807617188\n",
      "7 \t 268.67120361328125\n",
      "Cycle: \t 2\n",
      "****************************************\n",
      "2 \t 1.9682068824768066\n",
      "2 \t 0.4990575313568115\n",
      "2 \t 0.3897450566291809\n",
      "4 \t 12.952754974365234\n",
      "4 \t 7.651008129119873\n",
      "4 \t 6.384199142456055\n",
      "7 \t 275.5765075683594\n",
      "7 \t 217.23675537109375\n",
      "7 \t 257.0955810546875\n",
      "Cycle: \t 3\n",
      "****************************************\n",
      "2 \t 1.5159850120544434\n",
      "2 \t 0.4675881564617157\n",
      "2 \t 0.4602562189102173\n",
      "4 \t 12.953466415405273\n",
      "4 \t 2.3079302310943604\n",
      "4 \t 2.6705832481384277\n",
      "7 \t 136.86370849609375\n",
      "7 \t 162.65521240234375\n",
      "7 \t 114.81729125976562\n",
      "Cycle: \t 4\n",
      "****************************************\n",
      "2 \t 0.8298442959785461\n",
      "2 \t 0.46702754497528076\n",
      "2 \t 0.4157245457172394\n",
      "4 \t 14.93557071685791\n",
      "4 \t 4.661985397338867\n",
      "4 \t 1.2214040756225586\n",
      "7 \t 159.5845489501953\n",
      "7 \t 147.6610107421875\n",
      "7 \t 148.00247192382812\n",
      "Cycle: \t 5\n",
      "****************************************\n",
      "2 \t 1.055870532989502\n",
      "2 \t 0.5318610668182373\n",
      "2 \t 0.4835242033004761\n",
      "4 \t 9.929915428161621\n",
      "4 \t 4.164517402648926\n",
      "4 \t 1.1591196060180664\n",
      "7 \t 318.00787353515625\n",
      "7 \t 230.66949462890625\n",
      "7 \t 81.54682922363281\n",
      "Cycle: \t 6\n",
      "****************************************\n",
      "2 \t 0.6199349761009216\n",
      "2 \t 0.45795607566833496\n",
      "2 \t 0.2857949733734131\n",
      "4 \t 3.8812761306762695\n",
      "4 \t 2.536487579345703\n",
      "4 \t 2.3276848793029785\n",
      "7 \t 181.49188232421875\n",
      "7 \t 40.67869567871094\n",
      "7 \t 52.39356994628906\n",
      "Cycle: \t 7\n",
      "****************************************\n",
      "2 \t 1.1843509674072266\n",
      "2 \t 0.3907170295715332\n",
      "2 \t 0.20622344315052032\n",
      "4 \t 7.083623886108398\n",
      "4 \t 1.9026141166687012\n",
      "4 \t 1.5591062307357788\n",
      "7 \t 70.62721252441406\n",
      "7 \t 21.53912925720215\n",
      "7 \t 50.36164474487305\n",
      "Cycle: \t 8\n",
      "****************************************\n",
      "2 \t 0.6156113147735596\n",
      "2 \t 0.2338825911283493\n",
      "2 \t 0.1991686224937439\n",
      "4 \t 7.732969284057617\n",
      "4 \t 1.3314013481140137\n",
      "4 \t 1.8395967483520508\n",
      "7 \t 90.92838287353516\n",
      "7 \t 40.1549072265625\n",
      "7 \t 27.717723846435547\n",
      "Cycle: \t 9\n",
      "****************************************\n",
      "2 \t 0.5814833641052246\n",
      "2 \t 0.2983311712741852\n",
      "2 \t 0.1165185272693634\n",
      "4 \t 4.586206912994385\n",
      "4 \t 1.5706759691238403\n",
      "4 \t 2.023914098739624\n",
      "7 \t 126.633056640625\n",
      "7 \t 19.620075225830078\n",
      "7 \t 23.80678939819336\n",
      "__________________________________________________\n",
      "\n",
      "TRAINING LOSS ITERATION:  3\n",
      "2 \t 2.6544973850250244\n",
      "4 \t 16.418874740600586\n",
      "7 \t 137.05575561523438\n",
      "2 \t 1.434081792831421\n",
      "4 \t 9.063583374023438\n",
      "7 \t 41.26207733154297\n",
      "2 \t 1.5303057432174683\n",
      "4 \t 13.636857032775879\n",
      "7 \t 60.74982452392578\n",
      "2 \t 0.9658013582229614\n",
      "4 \t 9.240103721618652\n",
      "7 \t 33.7425537109375\n",
      "2 \t 1.087132215499878\n",
      "4 \t 5.446203708648682\n",
      "7 \t 31.504985809326172\n",
      "2 \t 0.34276482462882996\n",
      "4 \t 4.438100814819336\n",
      "7 \t 30.915163040161133\n",
      "2 \t 0.7863240838050842\n",
      "4 \t 8.410711288452148\n",
      "7 \t 61.948238372802734\n",
      "2 \t 0.42285749316215515\n",
      "4 \t 2.17924427986145\n",
      "7 \t 39.21885299682617\n",
      "2 \t 0.4062517285346985\n",
      "4 \t 7.958893775939941\n",
      "7 \t 19.392581939697266\n",
      "2 \t 0.5279147624969482\n",
      "4 \t 8.45655345916748\n",
      "7 \t 50.77180099487305\n",
      "__________________________________________________\n",
      "\n",
      "VALIDATION LOSS ITERATION:  3\n",
      "6 \t 0.03224027901887894\n",
      "8 \t 0.007809395901858807\n",
      "9 \t 0.004601665772497654\n",
      "2 \t 0.006617828272283077\n",
      "4 \t 0.02415034919977188\n",
      "7 \t 0.005436085630208254\n",
      "__________________________________________________\n",
      "\n",
      "ERROR POINTS:\n",
      "NEW SET: \n",
      "[0, 1, 2, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 41, 80, 81, 120, 121, 160, 161, 162, 164, 168, 172, 176, 180, 184, 188, 192, 196, 200, 201, 240, 241, 280, 281, 320, 321, 324, 328, 332, 336, 340, 344, 348, 352, 356, 360, 400, 440, 441, 480, 481, 484, 488, 492, 496, 500, 504, 508, 512, 516, 520, 521, 560, 561, 600, 601, 640, 641, 644, 648, 652, 656, 660, 664, 668, 672, 676, 680, 681, 720, 721, 760, 761, 800, 801, 802, 804, 808, 812, 816, 820, 824, 828, 832, 836, 840, 841, 881, 921, 960, 961, 962, 963, 964, 968, 972, 976, 980, 984, 988, 992, 996, 1001, 1120, 1124, 1128, 1132, 1136, 1140, 1144, 1148, 1152, 1156, 1280, 1284, 1288, 1292, 1296, 1300, 1304, 1308, 1312, 1316, 1440, 1444, 1448, 1452, 1456, 1460, 1464, 1468, 1472, 1476]\n",
      "\n",
      "_________________________________\n",
      "WARMUP TRAINING ITERATION:  4\n",
      "2 \t 164.98138427734375\n",
      "4 \t 155.30189514160156\n",
      "7 \t 1336.225830078125\n",
      "2 \t 13.234042167663574\n",
      "4 \t 72.82012939453125\n",
      "7 \t 775.790771484375\n",
      "2 \t 9.187950134277344\n",
      "4 \t 69.54994201660156\n",
      "7 \t 693.197998046875\n",
      "2 \t 6.527885913848877\n",
      "4 \t 60.48210525512695\n",
      "7 \t 561.8915405273438\n",
      "2 \t 6.985357761383057\n",
      "4 \t 45.940032958984375\n",
      "7 \t 531.6315307617188\n",
      "2 \t 5.262604713439941\n",
      "4 \t 44.449554443359375\n",
      "7 \t 440.02410888671875\n",
      "2 \t 5.679162502288818\n",
      "4 \t 28.879562377929688\n",
      "7 \t 489.22021484375\n",
      "2 \t 3.7809953689575195\n",
      "4 \t 38.51264190673828\n",
      "7 \t 425.05926513671875\n",
      "2 \t 4.427061080932617\n",
      "4 \t 39.49388885498047\n",
      "7 \t 371.5104064941406\n",
      "2 \t 3.393042802810669\n",
      "4 \t 30.362058639526367\n",
      "7 \t 300.9976806640625\n",
      "__________________________________________________\n",
      "\n",
      "TRAINING ITERATION:  4\n",
      "Cycle: \t 0\n",
      "****************************************\n",
      "2 \t 2.4570915699005127\n",
      "2 \t 1.8569645881652832\n",
      "2 \t 2.0830037593841553\n",
      "4 \t 32.08638000488281\n",
      "4 \t 20.28072166442871\n",
      "4 \t 8.820116996765137\n",
      "7 \t 504.8531494140625\n",
      "7 \t 272.9205627441406\n",
      "7 \t 290.95147705078125\n",
      "Cycle: \t 1\n",
      "****************************************\n",
      "2 \t 2.996023654937744\n",
      "2 \t 0.9644196033477783\n",
      "2 \t 0.7398861646652222\n",
      "4 \t 22.02405548095703\n",
      "4 \t 4.544517517089844\n",
      "4 \t 5.732611656188965\n",
      "7 \t 557.7904052734375\n",
      "7 \t 174.02615356445312\n",
      "7 \t 195.5235595703125\n",
      "Cycle: \t 2\n",
      "****************************************\n",
      "2 \t 2.188739061355591\n",
      "2 \t 0.6590205430984497\n",
      "2 \t 0.27675914764404297\n",
      "4 \t 12.938888549804688\n",
      "4 \t 5.173290252685547\n",
      "4 \t 5.239181995391846\n",
      "7 \t 369.1002197265625\n",
      "7 \t 222.13595581054688\n",
      "7 \t 146.9368438720703\n",
      "Cycle: \t 3\n",
      "****************************************\n",
      "2 \t 2.410362482070923\n",
      "2 \t 0.25615620613098145\n",
      "2 \t 0.23102015256881714\n",
      "4 \t 13.414344787597656\n",
      "4 \t 4.944636344909668\n",
      "4 \t 1.3860644102096558\n",
      "7 \t 294.19384765625\n",
      "7 \t 179.5467529296875\n",
      "7 \t 50.049564361572266\n",
      "Cycle: \t 4\n",
      "****************************************\n",
      "2 \t 1.2731274366378784\n",
      "2 \t 0.29676708579063416\n",
      "2 \t 0.14890806376934052\n",
      "4 \t 12.161598205566406\n",
      "4 \t 5.5532121658325195\n",
      "4 \t 2.4122657775878906\n",
      "7 \t 130.61871337890625\n",
      "7 \t 38.24510192871094\n",
      "7 \t 41.146541595458984\n",
      "Cycle: \t 5\n",
      "****************************************\n",
      "2 \t 0.7784625291824341\n",
      "2 \t 0.18555720150470734\n",
      "2 \t 0.1361352801322937\n",
      "4 \t 10.869878768920898\n",
      "4 \t 1.4425054788589478\n",
      "4 \t 6.176448345184326\n",
      "7 \t 105.91787719726562\n",
      "7 \t 22.333059310913086\n",
      "7 \t 26.213890075683594\n",
      "Cycle: \t 6\n",
      "****************************************\n",
      "2 \t 1.2244021892547607\n",
      "2 \t 0.30727988481521606\n",
      "2 \t 0.1424126923084259\n",
      "4 \t 5.8790483474731445\n",
      "4 \t 1.1915353536605835\n",
      "4 \t 1.3567172288894653\n",
      "7 \t 57.13616180419922\n",
      "7 \t 72.24008178710938\n",
      "7 \t 23.174535751342773\n",
      "Cycle: \t 7\n",
      "****************************************\n",
      "2 \t 0.7743860483169556\n",
      "2 \t 0.13125231862068176\n",
      "2 \t 0.11407309770584106\n",
      "4 \t 2.71028995513916\n",
      "4 \t 1.7841317653656006\n",
      "4 \t 0.8325022459030151\n",
      "7 \t 54.88016891479492\n",
      "7 \t 26.363601684570312\n",
      "7 \t 9.433592796325684\n",
      "Cycle: \t 8\n",
      "****************************************\n",
      "2 \t 0.6396762132644653\n",
      "2 \t 0.09188595414161682\n",
      "2 \t 0.32639920711517334\n",
      "4 \t 2.3726086616516113\n",
      "4 \t 0.9849290251731873\n",
      "4 \t 0.824567437171936\n",
      "7 \t 32.30757141113281\n",
      "7 \t 42.74962615966797\n",
      "7 \t 7.289917945861816\n",
      "Cycle: \t 9\n",
      "****************************************\n",
      "2 \t 0.2529282867908478\n",
      "2 \t 0.11105944961309433\n",
      "2 \t 0.13573484122753143\n",
      "4 \t 1.94083833694458\n",
      "4 \t 0.6260985136032104\n",
      "4 \t 0.8103387951850891\n",
      "7 \t 28.628868103027344\n",
      "7 \t 25.341352462768555\n",
      "7 \t 19.848148345947266\n",
      "__________________________________________________\n",
      "\n",
      "TRAINING LOSS ITERATION:  4\n",
      "2 \t 0.29709091782569885\n",
      "4 \t 4.6496148109436035\n",
      "7 \t 90.57652282714844\n",
      "2 \t 0.43423816561698914\n",
      "4 \t 3.393674373626709\n",
      "7 \t 54.21613311767578\n",
      "2 \t 0.6331772804260254\n",
      "4 \t 4.965372085571289\n",
      "7 \t 54.55410385131836\n",
      "2 \t 1.3550158739089966\n",
      "4 \t 5.007067680358887\n",
      "7 \t 29.595157623291016\n",
      "2 \t 0.23784296214580536\n",
      "4 \t 3.6673552989959717\n",
      "7 \t 38.42185592651367\n",
      "2 \t 0.12064244598150253\n",
      "4 \t 2.6236183643341064\n",
      "7 \t 60.88664627075195\n",
      "2 \t 0.35522544384002686\n",
      "4 \t 2.9702043533325195\n",
      "7 \t 42.098793029785156\n",
      "2 \t 0.30326491594314575\n",
      "4 \t 1.8525792360305786\n",
      "7 \t 23.760780334472656\n",
      "2 \t 0.8638663291931152\n",
      "4 \t 1.3747345209121704\n",
      "7 \t 17.75534439086914\n",
      "2 \t 0.20043402910232544\n",
      "4 \t 4.3431572914123535\n",
      "7 \t 26.86258888244629\n",
      "__________________________________________________\n",
      "\n",
      "VALIDATION LOSS ITERATION:  4\n",
      "6 \t 0.0008634494733996689\n",
      "8 \t 0.0008956460515037179\n",
      "9 \t 0.0008613465470261872\n",
      "2 \t 0.0024523541796952486\n",
      "4 \t 0.007748517207801342\n",
      "7 \t 0.007912203669548035\n",
      "__________________________________________________\n",
      "\n",
      "ERROR POINTS:\n",
      "NEW SET: \n",
      "[0, 1, 2, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 41, 42, 80, 81, 82, 120, 121, 122, 160, 161, 162, 164, 168, 172, 176, 180, 184, 188, 192, 196, 200, 201, 240, 241, 280, 281, 320, 321, 322, 324, 328, 332, 336, 340, 344, 348, 352, 356, 360, 361, 400, 401, 440, 441, 442, 480, 481, 482, 484, 488, 492, 496, 500, 504, 508, 512, 516, 520, 521, 522, 560, 561, 562, 600, 601, 640, 641, 644, 648, 652, 656, 660, 664, 668, 672, 676, 680, 681, 720, 721, 760, 761, 800, 801, 802, 804, 808, 812, 816, 820, 824, 828, 832, 836, 840, 841, 881, 921, 960, 961, 962, 963, 964, 968, 972, 976, 980, 984, 988, 992, 996, 1001, 1120, 1124, 1128, 1132, 1136, 1140, 1144, 1148, 1152, 1156, 1280, 1284, 1288, 1292, 1296, 1300, 1304, 1308, 1312, 1316, 1440, 1444, 1448, 1452, 1456, 1460, 1464, 1468, 1472, 1476]\n",
      "\n",
      "_________________________________\n"
     ]
    }
   ],
   "source": [
    "for learning_set in range (5):\n",
    "\n",
    "    training_data_2 = auto_encoder.get_dataset_active(data_2,2,1600, pts)\n",
    "    training_data_4 = auto_encoder.get_dataset_active(data_4,4,1600, pts)\n",
    "    training_data_7 = auto_encoder.get_dataset_active(data_7,7,1600, pts) \n",
    "\n",
    "\n",
    "    split1 = int(len(pts)*0.9)\n",
    "    split2 = int(len(pts)/10)\n",
    "    split1 += len(pts)-(split1+split2)\n",
    "    \n",
    "    training_data_2, val_data_2 = random_split(training_data_2, [split1,split2])\n",
    "    training_data_4, val_data_4 = random_split(training_data_4, [split1,split2])\n",
    "    training_data_7, val_data_7 = random_split(training_data_7, [split1,split2])\n",
    "\n",
    "    datasets = [training_data_2,\n",
    "                training_data_4,\n",
    "                training_data_7]\n",
    "\n",
    "    training_loaders = [DataLoader(x, batch_size = 32,  shuffle=True, num_workers=20) for x in datasets]\n",
    "\n",
    "    val_data_6 = get_dataset(data_6, 6, 1600)\n",
    "    val_data_8 = get_dataset(data_8, 8, 1600)\n",
    "    val_data_9 = get_dataset(data_9, 9, 1600)\n",
    "\n",
    "    val_datasets = [val_data_6, val_data_8, val_data_9, val_data_2, val_data_4, val_data_7]\n",
    "\n",
    "    val_loaders = [DataLoader(x, batch_size = 10, num_workers=20) for x in val_datasets]\n",
    "\n",
    "    warmup_2 = next(iter(training_loaders[0]))\n",
    "    warmup_4 = next(iter(training_loaders[1]))\n",
    "    warmup_7 = next(iter(training_loaders[2]))\n",
    "\n",
    "    mps_size = 5\n",
    "    model = auto_encoder.MPS_autoencoder(mps_size = mps_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_func = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    warmup_data = [(warmup_2,2), (warmup_4,4), (warmup_7,7)]\n",
    "\n",
    "    print(\"WARMUP TRAINING ITERATION: \", learning_set)\n",
    "    for j in range(10):\n",
    "        for i in range(3):\n",
    "            for epoch in range(10):\n",
    "                fields,wf = warmup_data[i][0]\n",
    "                gs = model(fields, warmup_data[i][1])            \n",
    "                loss = loss_func(gs, wf)\n",
    "                if (epoch % 10 == 0):\n",
    "                    current_loss = loss.item() *(2**warmup_data[i][1])\n",
    "                    print(warmup_data[i][1],\"\\t\", current_loss)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if(j==9):\n",
    "                final_warmup_loss[learning_set][i] = current_loss\n",
    "    print(\"__________________________________________________\")\n",
    "    print()\n",
    "\n",
    "    print(\"TRAINING ITERATION: \", learning_set)\n",
    "    for j in range(10):\n",
    "        print(\"Cycle: \\t\", j)\n",
    "        print(\"*\"*40)\n",
    "        for i in range(3):\n",
    "            for epoch in range(201):\n",
    "                fields,wf = warmup_data[i][0]\n",
    "                gs = model(fields, warmup_data[i][1])            \n",
    "                loss = loss_func(gs, wf)\n",
    "                if (epoch % 100 == 0):\n",
    "                    current_loss = loss.item() *(2**warmup_data[i][1])\n",
    "                    print(warmup_data[i][1],\"\\t\", current_loss)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if(j==9):\n",
    "                final_training_epoch_loss[learning_set][i] = current_loss\n",
    "    print(\"__________________________________________________\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    warmup_2 = next(iter(training_loaders[0]))\n",
    "    warmup_4 = next(iter(training_loaders[1]))\n",
    "    warmup_7 = next(iter(training_loaders[2]))\n",
    "    warmup_data = [(warmup_2,2), (warmup_4,4), (warmup_7,7)]\n",
    "\n",
    "    print(\"TRAINING LOSS ITERATION: \", learning_set)\n",
    "    for j in range(10):\n",
    "        for i in range(3):\n",
    "            for epoch in range(10):\n",
    "                fields,wf = warmup_data[i][0]\n",
    "                gs = model(fields, warmup_data[i][1])            \n",
    "                loss = loss_func(gs, wf)\n",
    "                if (epoch % 10 == 0):\n",
    "                    current_loss = loss.item() *(2**warmup_data[i][1])\n",
    "                    print(warmup_data[i][1],\"\\t\", current_loss)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if(j==9):\n",
    "                final_training_loss[learning_set][i] = current_loss        \n",
    "    print(\"__________________________________________________\")\n",
    "    print()\n",
    "\n",
    "    warmup_data = [(warmup_2,2), (warmup_4,4), (warmup_7,7)]\n",
    "    f = open(\"warmup_data_2.p\", 'wb')\n",
    "    pickle.dump(warmup_data, f)\n",
    "    f.close()\n",
    "\n",
    "    f = open(\"warmup_data_1.p\", 'wb')\n",
    "    pickle.dump(warmup_data, f)\n",
    "    f.close()\n",
    "\n",
    "    val_6 = next(iter(val_loaders[0]))\n",
    "    val_8 = next(iter(val_loaders[1]))\n",
    "    val_9 = next(iter(val_loaders[2]))\n",
    "    val_2 = next(iter(val_loaders[3]))\n",
    "    val_4 = next(iter(val_loaders[4]))\n",
    "    val_7 = next(iter(val_loaders[5]))\n",
    "\n",
    "    print(\"VALIDATION LOSS ITERATION: \", learning_set)\n",
    "\n",
    "    val_data = [(val_6,6),(val_8,8),(val_9,9),(val_2,2),(val_4,4),(val_7,7)]\n",
    "    loss_func = nn.MSELoss()\n",
    "    count=0\n",
    "    for data, size in val_data:\n",
    "        with torch.no_grad():\n",
    "            fields, wf = data\n",
    "            gs = model(fields, size)\n",
    "            loss = loss_func(gs,wf)\n",
    "            current = loss.item() * (2**size)\n",
    "            print(size,\"\\t\" ,current)\n",
    "            final_validation_loss[learning_set][count] = current\n",
    "        count+=1\n",
    "\n",
    "    print(\"__________________________________________________\")\n",
    "    print()\n",
    "\n",
    "    f = open(\"test_data_1.p\", 'wb')\n",
    "    pickle.dump(val_data, f)\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    for N, train_loader in enumerate(training_loaders):            \n",
    "        temp = 0\n",
    "        sys_size = training_n_sizes[N]\n",
    "        for i, (fields,wf) in enumerate(train_loader):\n",
    "            fields = fields.to(device)\n",
    "            gs = model(fields, sys_size)            \n",
    "            loss = loss_func(gs, wf.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            temp += loss.item()\n",
    "        temp = temp / (len(train_loader)) \n",
    "\n",
    "    data_sizes = [2,4,6,7,8,9]\n",
    "    training_data_2 = get_dataset(data_2, 2, 1600)\n",
    "    training_data_4 = get_dataset(data_4,4,1600)\n",
    "    training_data_7 = get_dataset(data_7,7,1600) \n",
    "    val_data_6 = get_dataset(data_6, 6, 1600)\n",
    "    val_data_8 = get_dataset(data_8, 8, 1600)\n",
    "    val_data_9 = get_dataset(data_9, 9, 1600)\n",
    "    mag_dat = [training_data_2,training_data_4,val_data_6,training_data_7, val_data_8, val_data_9]\n",
    "    mag_loaders = [DataLoader(x, batch_size = len(val_data_6), num_workers=20) for x in mag_dat]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_systems = {}\n",
    "        for j,loader in enumerate((mag_loaders)):\n",
    "            sys_size = data_sizes[j]\n",
    "            wave_functions = []\n",
    "            true_wave = []\n",
    "            for i, (fields,wf) in enumerate(loader):\n",
    "                fields = fields\n",
    "                gs = model(fields, sys_size)\n",
    "\n",
    "                wave_functions.append(gs)\n",
    "                true_wave.append(wf)\n",
    "\n",
    "            n_systems[sys_size] = (wave_functions,true_wave)\n",
    "\n",
    "\n",
    "#     data_y_2 = n_systems[2][0][0].numpy()\n",
    "#     data_y_4 = n_systems[4][0][0].numpy()\n",
    "#     data_y_7 = n_systems[7][0][0].numpy()\n",
    "    data_y_6 = n_systems[6][0][0].numpy()\n",
    "#     data_y_8 = n_systems[8][0][0].numpy()\n",
    "#     data_y_9 = n_systems[9][0][0].numpy()\n",
    "\n",
    "\n",
    "    #     print(data_y_2.shape)\n",
    "#     data_y_2_t = n_systems[2][1][0].numpy()\n",
    "#     data_y_4_t = n_systems[4][1][0].numpy()\n",
    "#     data_y_7_t = n_systems[7][1][0].numpy()\n",
    "    data_y_6_t = n_systems[6][1][0].numpy()\n",
    "#     data_y_8_t = n_systems[8][1][0].numpy()\n",
    "#     data_y_9_t = n_systems[9][1][0].numpy()\n",
    "\n",
    "#     vec = seq_to_magnetization(seq_gen(2),2)\n",
    "#     vec_2 = (vec.view()).reshape((4,1))\n",
    "#     mag_2 = np.squeeze((np.power(data_y_2,2) @ vec_2))\n",
    "#     mag_2_t = np.squeeze((np.power(data_y_2_t,2) @ vec_2))\n",
    "\n",
    "#     vec_4 = seq_to_magnetization(seq_gen(4),4).reshape((16,1))\n",
    "#     mag_4 = np.squeeze((np.power(data_y_4,2) @ vec_4))\n",
    "#     mag_4_t = np.squeeze((np.power(data_y_4_t,2) @ vec_4))\n",
    "\n",
    "    vec_6 = seq_to_magnetization(seq_gen(6),6).reshape((64,1))\n",
    "    mag_6 = np.squeeze((np.power(data_y_6,2) @ vec_6))\n",
    "    mag_6_t = np.squeeze((np.power(data_y_6_t,2) @ vec_6))\n",
    "    \n",
    "    magnetization_6.append(mag_6)\n",
    "\n",
    "#     vec_7 = seq_to_magnetization(seq_gen(7),7).reshape((128,1))\n",
    "#     mag_7 = np.squeeze((np.power(data_y_7,2) @ vec_7))\n",
    "#     mag_7_t = np.squeeze((np.power(data_y_7_t,2) @ vec_7))\n",
    "\n",
    "#     vec_8 = seq_to_magnetization(seq_gen(8),8).reshape((256,1))\n",
    "#     mag_8 = np.squeeze((np.power(data_y_8,2) @ vec_8))\n",
    "#     mag_8_t = np.squeeze((np.power(data_y_8_t,2) @ vec_8))\n",
    "\n",
    "#     vec_9 = seq_to_magnetization(seq_gen(9),9).reshape((512,1))\n",
    "#     mag_9 = np.squeeze((np.power(data_y_9,2) @ vec_9))\n",
    "#     mag_9_t = np.squeeze((np.power(data_y_9_t,2) @ vec_9))\n",
    "\n",
    "    error_pts = error_points(mag_6, mag_6_t, 10)\n",
    "    print(\"ERROR POINTS:\")\n",
    "    print_points(data_6, 6, error_pts)\n",
    "\n",
    "\n",
    "    pts = expand_pool(pts, error_pts, 1600)\n",
    "    print(\"NEW SET: \")\n",
    "    print(pts)\n",
    "    print()\n",
    "    print(\"_________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73616ed2-5053-49b7-8ceb-237ee8738e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Warmup Loss:\n",
      "2 :\n",
      "set 0 : 5.717419624328613\n",
      "set 1 : 5.296435356140137\n",
      "set 2 : 4.756293296813965\n",
      "set 3 : 6.357902526855469\n",
      "set 4 : 3.393042802810669\n",
      "4 :\n",
      "set 0 : 29.40480613708496\n",
      "set 1 : 30.95516014099121\n",
      "set 2 : 51.92731475830078\n",
      "set 3 : 29.607013702392578\n",
      "set 4 : 30.362058639526367\n",
      "7 :\n",
      "set 0 : 439.81695556640625\n",
      "set 1 : 601.9066162109375\n",
      "set 2 : 385.39276123046875\n",
      "set 3 : 753.9805297851562\n",
      "set 4 : 300.9976806640625\n",
      "\n",
      "Final Training Epoch Loss:\n",
      "2 :\n",
      "set 0 : 0.10216493904590607\n",
      "set 1 : 0.2483154833316803\n",
      "set 2 : 0.17460453510284424\n",
      "set 3 : 0.1165185272693634\n",
      "set 4 : 0.13573484122753143\n",
      "4 :\n",
      "set 0 : 9.85230827331543\n",
      "set 1 : 2.2272751331329346\n",
      "set 2 : 1.2546086311340332\n",
      "set 3 : 2.023914098739624\n",
      "set 4 : 0.8103387951850891\n",
      "7 :\n",
      "set 0 : 7.183125972747803\n",
      "set 1 : 74.34280395507812\n",
      "set 2 : 13.207186698913574\n",
      "set 3 : 23.80678939819336\n",
      "set 4 : 19.848148345947266\n",
      "\n",
      "Final Training Loss:\n",
      "2 :\n",
      "set 0 : 0.23229581117630005\n",
      "set 1 : 0.3679203987121582\n",
      "set 2 : 0.4751667380332947\n",
      "set 3 : 0.5279147624969482\n",
      "set 4 : 0.20043402910232544\n",
      "4 :\n",
      "set 0 : 1.306187629699707\n",
      "set 1 : 4.875025749206543\n",
      "set 2 : 4.589102745056152\n",
      "set 3 : 8.45655345916748\n",
      "set 4 : 4.3431572914123535\n",
      "7 :\n",
      "set 0 : 27.36439323425293\n",
      "set 1 : 88.27371978759766\n",
      "set 2 : 41.89576721191406\n",
      "set 3 : 50.77180099487305\n",
      "set 4 : 26.86258888244629\n",
      "\n",
      "Final Validation Loss:\n",
      "6 :\n",
      "set 0 : 0\n",
      "set 1 : 0\n",
      "set 2 : 0\n",
      "set 3 : 0\n",
      "set 4 : 0\n",
      "8 :\n",
      "set 0 : 0\n",
      "set 1 : 0\n",
      "set 2 : 0\n",
      "set 3 : 0\n",
      "set 4 : 0\n",
      "9 :\n",
      "set 0 : 0\n",
      "set 1 : 0\n",
      "set 2 : 0\n",
      "set 3 : 0\n",
      "set 4 : 0\n",
      "2 :\n",
      "set 0 : 0\n",
      "set 1 : 0\n",
      "set 2 : 0\n",
      "set 3 : 0\n",
      "set 4 : 0\n",
      "4 :\n",
      "set 0 : 0\n",
      "set 1 : 0\n",
      "set 2 : 0\n",
      "set 3 : 0\n",
      "set 4 : 0\n",
      "7 :\n",
      "set 0 : 0\n",
      "set 1 : 0\n",
      "set 2 : 0\n",
      "set 3 : 0\n",
      "set 4 : 0\n"
     ]
    }
   ],
   "source": [
    "qubit_nums = [2,4,7]\n",
    "\n",
    "print(\"Final Warmup Loss:\")\n",
    "for i in range (3):\n",
    "    print(qubit_nums[i], \":\")\n",
    "    for j in range (5):\n",
    "        print(\"set\", j,\":\", final_warmup_loss[j][i])\n",
    "        \n",
    "print()\n",
    "print(\"Final Training Epoch Loss:\")\n",
    "for i in range (3):\n",
    "    print(qubit_nums[i], \":\")\n",
    "    for j in range (5):\n",
    "        print(\"set\", j,\":\", final_training_epoch_loss[j][i])        \n",
    "\n",
    "print()\n",
    "print(\"Final Training Loss:\")\n",
    "for i in range (3):\n",
    "    print(qubit_nums[i], \":\")\n",
    "    for j in range (5):\n",
    "        print(\"set\", j,\":\", final_training_loss[j][i]) \n",
    "        \n",
    "qubit_nums = [6,8,9,2,4,7]\n",
    "print()\n",
    "print(\"Final Validation Loss:\")\n",
    "for i in range (6):\n",
    "    print(qubit_nums[i], \":\")\n",
    "    for j in range (5):\n",
    "        print(\"set\", j,\":\", final_validation_loss[j][i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e77ce31-06cb-4aa1-9e33-38e911e0f96d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1000 into shape (40,40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-cc64a41deac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagnetization_6\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/home/jszacha1/.conda/envs/conda_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    297\u001b[0m            [5, 6]])\n\u001b[1;32m    298\u001b[0m     \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jszacha1/.conda/envs/conda_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1000 into shape (40,40)"
     ]
    }
   ],
   "source": [
    "data_graph = np.load(data_6)\n",
    "Bx = data_graph['fields'].T[6]\n",
    "Bz = data_graph['fields'].T[12]\n",
    "\n",
    "x = np.reshape(Bx, (40, 40))\n",
    "y = np.reshape(Bz, (40, 40))\n",
    "z = np.reshape(mag_6_t, (40, 40))\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot_surface(x, y, z, cmap=\"binary\")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(\"Ground truth Magnetization\")\n",
    "ax.set_xlabel(\"Bx\")\n",
    "ax.set_ylabel(\"Bz\")\n",
    "ax.set_zlabel(\"Magnetization\")\n",
    "\n",
    "for i in range (5):\n",
    "    z = np.reshape(magnetization_6[i], (40, 40))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.plot_surface(x, y, z, cmap=\"binary\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Predicted Magnetization Set\", i)\n",
    "    ax.set_xlabel(\"Bx\")\n",
    "    ax.set_ylabel(\"Bz\")\n",
    "    ax.set_zlabel(\"Magnetization\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1b2d6b6-f2cd-439f-ab64-4b8fba2b8e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-005e50cf5892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_data_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauto_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset_active\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/research/MPSML/code/mps/auto_encoder.py\u001b[0m in \u001b[0;36mget_dataset_active\u001b[0;34m(fname, num_qubits, num_samples, data_pts)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_pts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ground_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_data_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "print(len(pts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea4f5d8b-0bf3-4c97-82f6-0635bc9f5ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "print(len(pts2))\n",
    "training_data_2 = auto_encoder.get_dataset_active(data_2,2,1600, pts2)\n",
    "training_data_4 = auto_encoder.get_dataset_active(data_4,4,1600, pts2)\n",
    "training_data_7 = auto_encoder.get_dataset_active(data_7,7,1600, pts2) \n",
    "\n",
    "training_data_2, val_data_2 = random_split(training_data_2, [100,10])\n",
    "training_data_4, val_data_4 = random_split(training_data_4, [100,10])\n",
    "training_data_7, val_data_7 = random_split(training_data_7, [100,10])\n",
    "\n",
    "datasets = [training_data_2,\n",
    "            training_data_4,\n",
    "            training_data_7]\n",
    "\n",
    "training_loaders = [DataLoader(x, batch_size = 32,  shuffle=True, num_workers=20) for x in datasets]\n",
    "\n",
    "val_datasets = [val_data_6, val_data_8, val_data_9, val_data_2, val_data_4, val_data_7]\n",
    "\n",
    "val_loaders = [DataLoader(x, batch_size = 10, num_workers=20) for x in val_datasets]\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e9f1f-bfda-4534-9f90-9d2c33fbaf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_2 = next(iter(training_loaders[0]))\n",
    "warmup_4 = next(iter(training_loaders[1]))\n",
    "warmup_7 = next(iter(training_loaders[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7626fcc4-1b9b-49a1-be13-c9b386acea5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \t 180.05601501464844\n",
      "4 \t 396.9764709472656\n",
      "7 \t 10215.5751953125\n",
      "2 \t 18.95096206665039\n",
      "4 \t 89.82289123535156\n",
      "7 \t 1120.547119140625\n",
      "2 \t 9.390539169311523\n",
      "4 \t 114.86370849609375\n",
      "7 \t 665.7327270507812\n",
      "2 \t 9.205703735351562\n",
      "4 \t 27.10988998413086\n",
      "7 \t 541.2842407226562\n",
      "2 \t 6.068642616271973\n",
      "4 \t 31.623220443725586\n",
      "7 \t 602.8790283203125\n",
      "2 \t 3.5196595191955566\n",
      "4 \t 32.66358947753906\n",
      "7 \t 555.59814453125\n",
      "2 \t 5.390881538391113\n",
      "4 \t 32.55101776123047\n",
      "7 \t 357.78985595703125\n",
      "2 \t 5.217724800109863\n",
      "4 \t 24.43670082092285\n",
      "7 \t 423.5292663574219\n",
      "2 \t 2.31003475189209\n",
      "4 \t 17.998199462890625\n",
      "7 \t 398.8067932128906\n",
      "2 \t 3.052884101867676\n",
      "4 \t 25.578933715820312\n",
      "7 \t 532.6058349609375\n"
     ]
    }
   ],
   "source": [
    "mps_size = 5\n",
    "model = auto_encoder.MPS_autoencoder(mps_size = mps_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_func = nn.MSELoss(reduction='sum')\n",
    "\n",
    "warmup_data = [(warmup_2,2), (warmup_4,4), (warmup_7,7)]\n",
    "\n",
    "for j in range(10):\n",
    "    for i in range(3):\n",
    "        for epoch in range(10):\n",
    "            fields,wf = warmup_data[i][0]\n",
    "            gs = model(fields, warmup_data[i][1])            \n",
    "            loss = loss_func(gs, wf)\n",
    "            if (epoch % 10 == 0):\n",
    "                print(warmup_data[i][1],\"\\t\", loss.item() *(2**warmup_data[i][1]))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ea56acef-4245-4c52-9399-2b8f479c9d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle: \t 0\n",
      "****************************************\n",
      "2 \t 2.7821240425109863\n",
      "2 \t 0.8656794428825378\n",
      "2 \t 1.0361649990081787\n",
      "4 \t 10.10662841796875\n",
      "4 \t 6.433604717254639\n",
      "4 \t 9.843862533569336\n",
      "7 \t 314.19775390625\n",
      "7 \t 178.2550506591797\n",
      "7 \t 129.41354370117188\n",
      "Cycle: \t 1\n",
      "****************************************\n",
      "2 \t 1.6743013858795166\n",
      "2 \t 0.5070786476135254\n",
      "2 \t 0.5475218296051025\n",
      "4 \t 5.278177261352539\n",
      "4 \t 1.4191985130310059\n",
      "4 \t 2.4135565757751465\n",
      "7 \t 166.12510681152344\n",
      "7 \t 126.17274475097656\n",
      "7 \t 114.77327728271484\n",
      "Cycle: \t 2\n",
      "****************************************\n",
      "2 \t 1.3151390552520752\n",
      "2 \t 0.4743051528930664\n",
      "2 \t 0.2869024872779846\n",
      "4 \t 3.0823166370391846\n",
      "4 \t 2.003108263015747\n",
      "4 \t 6.410835266113281\n",
      "7 \t 159.98497009277344\n",
      "7 \t 152.13739013671875\n",
      "7 \t 107.46759033203125\n",
      "Cycle: \t 3\n",
      "****************************************\n",
      "2 \t 0.7299072742462158\n",
      "2 \t 0.1635984480381012\n",
      "2 \t 0.23090417683124542\n",
      "4 \t 3.750051975250244\n",
      "4 \t 1.2644503116607666\n",
      "4 \t 1.6398191452026367\n",
      "7 \t 150.90695190429688\n",
      "7 \t 107.52298736572266\n",
      "7 \t 105.24052429199219\n",
      "Cycle: \t 4\n",
      "****************************************\n",
      "2 \t 1.3412237167358398\n",
      "2 \t 0.21694040298461914\n",
      "2 \t 0.5675799250602722\n",
      "4 \t 1.7102984189987183\n",
      "4 \t 0.7130144834518433\n",
      "4 \t 0.4793955981731415\n",
      "7 \t 193.12460327148438\n",
      "7 \t 99.69441223144531\n",
      "7 \t 101.5985107421875\n",
      "Cycle: \t 5\n",
      "****************************************\n",
      "2 \t 1.76131010055542\n",
      "2 \t 0.27691417932510376\n",
      "2 \t 0.129681795835495\n",
      "4 \t 2.5785467624664307\n",
      "4 \t 0.773235559463501\n",
      "4 \t 0.4064834415912628\n",
      "7 \t 221.10748291015625\n",
      "7 \t 104.96895599365234\n",
      "7 \t 98.47705078125\n",
      "Cycle: \t 6\n",
      "****************************************\n",
      "2 \t 1.0809895992279053\n",
      "2 \t 0.24028730392456055\n",
      "2 \t 0.19777099788188934\n",
      "4 \t 1.3701605796813965\n",
      "4 \t 0.889988362789154\n",
      "4 \t 0.3155291676521301\n",
      "7 \t 205.70083618164062\n",
      "7 \t 84.67523193359375\n",
      "7 \t 99.1255874633789\n",
      "Cycle: \t 7\n",
      "****************************************\n",
      "2 \t 0.9780494570732117\n",
      "2 \t 0.1254585087299347\n",
      "2 \t 0.12308631837368011\n",
      "4 \t 1.719335913658142\n",
      "4 \t 0.36559200286865234\n",
      "4 \t 0.40077561140060425\n",
      "7 \t 169.37576293945312\n",
      "7 \t 106.27427673339844\n",
      "7 \t 98.71111297607422\n",
      "Cycle: \t 8\n",
      "****************************************\n",
      "2 \t 0.8026938438415527\n",
      "2 \t 0.48639488220214844\n",
      "2 \t 0.07060009241104126\n",
      "4 \t 2.0002458095550537\n",
      "4 \t 0.34414905309677124\n",
      "4 \t 0.42472657561302185\n",
      "7 \t 208.13827514648438\n",
      "7 \t 83.53419494628906\n",
      "7 \t 92.52670288085938\n",
      "Cycle: \t 9\n",
      "****************************************\n",
      "2 \t 0.5584667325019836\n",
      "2 \t 0.12735708057880402\n",
      "2 \t 1.5027788877487183\n",
      "4 \t 0.9026567935943604\n",
      "4 \t 0.4310857951641083\n",
      "4 \t 0.29640960693359375\n",
      "7 \t 192.54241943359375\n",
      "7 \t 88.03528594970703\n",
      "7 \t 110.21994018554688\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    print(\"Cycle: \\t\", j)\n",
    "    print(\"*\"*40)\n",
    "    for i in range(3):\n",
    "        for epoch in range(201):\n",
    "            fields,wf = warmup_data[i][0]\n",
    "            gs = model(fields, warmup_data[i][1])            \n",
    "            loss = loss_func(gs, wf)\n",
    "            if (epoch % 100 == 0):\n",
    "                print(warmup_data[i][1],\"\\t\", loss.item()*(2**warmup_data[i][1]))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33826925-0c1d-46a9-bf0d-3df0b7afc318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \t 3.7701163291931152\n",
      "4 \t 23.84670639038086\n",
      "7 \t 270.21173095703125\n",
      "2 \t 3.244191884994507\n",
      "4 \t 11.059944152832031\n",
      "7 \t 211.09796142578125\n",
      "2 \t 1.608811378479004\n",
      "4 \t 8.963644027709961\n",
      "7 \t 148.74935913085938\n",
      "2 \t 0.7763664126396179\n",
      "4 \t 6.531449317932129\n",
      "7 \t 167.21124267578125\n",
      "2 \t 1.0262962579727173\n",
      "4 \t 2.948197603225708\n",
      "7 \t 222.29953002929688\n",
      "2 \t 0.8547028303146362\n",
      "4 \t 6.0203752517700195\n",
      "7 \t 113.70122528076172\n",
      "2 \t 0.8155598640441895\n",
      "4 \t 2.4043195247650146\n",
      "7 \t 213.64430236816406\n",
      "2 \t 1.0712642669677734\n",
      "4 \t 9.077162742614746\n",
      "7 \t 167.03335571289062\n",
      "2 \t 0.9819191098213196\n",
      "4 \t 7.032845497131348\n",
      "7 \t 166.7865753173828\n",
      "2 \t 0.6856197118759155\n",
      "4 \t 2.5456082820892334\n",
      "7 \t 124.79643249511719\n"
     ]
    }
   ],
   "source": [
    "warmup_2 = next(iter(training_loaders[0]))\n",
    "warmup_4 = next(iter(training_loaders[1]))\n",
    "warmup_7 = next(iter(training_loaders[2]))\n",
    "warmup_data = [(warmup_2,2), (warmup_4,4), (warmup_7,7)]\n",
    "\n",
    "\n",
    "for j in range(10):\n",
    "    for i in range(3):\n",
    "        for epoch in range(10):\n",
    "            fields,wf = warmup_data[i][0]\n",
    "            gs = model(fields, warmup_data[i][1])            \n",
    "            loss = loss_func(gs, wf)\n",
    "            if (epoch % 10 == 0):\n",
    "                print(warmup_data[i][1],\"\\t\", loss.item() *(2**warmup_data[i][1]))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "901ca4f6-6c03-428a-ae92-d786a9cfee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "warmup_data = [(warmup_2,2), (warmup_4,4), (warmup_7,7)]\n",
    "f = open(\"warmup_data_2.p\", 'wb')\n",
    "pickle.dump(warmup_data, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5d07107f-9e48-4a31-a8c7-d00fcc09e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"warmup_data_1.p\", 'wb')\n",
    "pickle.dump(warmup_data, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8b489a6a-d244-475c-8572-b13429ade3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_6 = next(iter(val_loaders[0]))\n",
    "val_8 = next(iter(val_loaders[1]))\n",
    "val_9 = next(iter(val_loaders[2]))\n",
    "val_2 = next(iter(val_loaders[3]))\n",
    "val_4 = next(iter(val_loaders[4]))\n",
    "val_7 = next(iter(val_loaders[5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fa5af75d-f2ee-45c7-8168-3d7ac2797df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 \t 0.07589209824800491\n",
      "8 \t 0.026231374591588974\n",
      "9 \t 0.033223532140254974\n",
      "2 \t 0.003109519137069583\n",
      "4 \t 0.055870283395051956\n",
      "7 \t 0.0051779476925730705\n"
     ]
    }
   ],
   "source": [
    "val_data = [(val_6,6),(val_8,8),(val_9,9),(val_2,2),(val_4,4),(val_7,7)]\n",
    "loss_func = nn.MSELoss()\n",
    "for data, size in val_data:\n",
    "    with torch.no_grad():\n",
    "        fields, wf = data\n",
    "        gs = model(fields, size)\n",
    "        loss = loss_func(gs,wf)\n",
    "        print(size,\"\\t\" ,loss.item() * (2**size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "86a9c616-12c8-47d7-b4d1-31cfcf95a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"test_data_1.p\", 'wb')\n",
    "pickle.dump(val_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a7e20d01-4a84-42e1-ba94-bb48642fa79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "for N, train_loader in enumerate(training_loaders):            \n",
    "    temp = 0\n",
    "    sys_size = training_n_sizes[N]\n",
    "    for i, (fields,wf) in enumerate(train_loader):\n",
    "        fields = fields.to(device)\n",
    "        gs = model(fields, sys_size)            \n",
    "        loss = loss_func(gs, wf.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        temp += loss.item()\n",
    "    temp = temp / (len(train_loader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b251a6fa-1593-4644-b2f3-bee5ebea1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    n_systems = {}\n",
    "    for j,loader in enumerate((mag_loaders)):\n",
    "        sys_size = data_sizes[j]\n",
    "        wave_functions = []\n",
    "        true_wave = []\n",
    "        for i, (fields,wf) in enumerate(loader):\n",
    "            fields = fields\n",
    "            gs = model(fields, sys_size)\n",
    "\n",
    "            wave_functions.append(gs)\n",
    "            true_wave.append(wf)\n",
    "\n",
    "        n_systems[sys_size] = (wave_functions,true_wave)\n",
    "\n",
    "\n",
    "data_y_2 = n_systems[2][0][0].numpy()\n",
    "data_y_4 = n_systems[4][0][0].numpy()\n",
    "data_y_7 = n_systems[7][0][0].numpy()\n",
    "data_y_6 = n_systems[6][0][0].numpy()\n",
    "data_y_8 = n_systems[8][0][0].numpy()\n",
    "data_y_9 = n_systems[9][0][0].numpy()\n",
    "\n",
    "\n",
    "#     print(data_y_2.shape)\n",
    "data_y_2_t = n_systems[2][1][0].numpy()\n",
    "data_y_4_t = n_systems[4][1][0].numpy()\n",
    "data_y_7_t = n_systems[7][1][0].numpy()\n",
    "data_y_6_t = n_systems[6][1][0].numpy()\n",
    "data_y_8_t = n_systems[8][1][0].numpy()\n",
    "data_y_9_t = n_systems[9][1][0].numpy()\n",
    "\n",
    "vec = seq_to_magnetization(seq_gen(2),2)\n",
    "vec_2 = (vec.view()).reshape((4,1))\n",
    "mag_2 = np.squeeze((np.power(data_y_2,2) @ vec_2))\n",
    "mag_2_t = np.squeeze((np.power(data_y_2_t,2) @ vec_2))\n",
    "\n",
    "vec_4 = seq_to_magnetization(seq_gen(4),4).reshape((16,1))\n",
    "mag_4 = np.squeeze((np.power(data_y_4,2) @ vec_4))\n",
    "mag_4_t = np.squeeze((np.power(data_y_4_t,2) @ vec_4))\n",
    "\n",
    "vec_6 = seq_to_magnetization(seq_gen(6),6).reshape((64,1))\n",
    "mag_6 = np.squeeze((np.power(data_y_6,2) @ vec_6))\n",
    "mag_6_t = np.squeeze((np.power(data_y_6_t,2) @ vec_6))\n",
    "\n",
    "vec_7 = seq_to_magnetization(seq_gen(7),7).reshape((128,1))\n",
    "mag_7 = np.squeeze((np.power(data_y_7,2) @ vec_7))\n",
    "mag_7_t = np.squeeze((np.power(data_y_7_t,2) @ vec_7))\n",
    "\n",
    "vec_8 = seq_to_magnetization(seq_gen(8),8).reshape((256,1))\n",
    "mag_8 = np.squeeze((np.power(data_y_8,2) @ vec_8))\n",
    "mag_8_t = np.squeeze((np.power(data_y_8_t,2) @ vec_8))\n",
    "\n",
    "vec_9 = seq_to_magnetization(seq_gen(9),9).reshape((512,1))\n",
    "mag_9 = np.squeeze((np.power(data_y_9,2) @ vec_9))\n",
    "mag_9_t = np.squeeze((np.power(data_y_9_t,2) @ vec_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ea4dcf34-4474-4433-9316-1634b8792974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[561 521 601 481 641 441 681 401 361 721]\n",
      "1600\n",
      "[(0.717948717948718, 0.05128205128205128), (0.6666666666666666, 0.05128205128205128), (0.7692307692307692, 0.05128205128205128), (0.6153846153846154, 0.05128205128205128), (0.8205128205128205, 0.05128205128205128), (0.5641025641025641, 0.05128205128205128), (0.8717948717948718, 0.05128205128205128), (0.5128205128205128, 0.05128205128205128), (0.4615384615384615, 0.05128205128205128), (0.923076923076923, 0.05128205128205128)]\n"
     ]
    }
   ],
   "source": [
    "error_pts = error_points(mag_6, mag_6_t, 10)\n",
    "print(error_pts)\n",
    "\n",
    "qubits=6\n",
    "data = np.load(data_6)\n",
    "Bx = data['fields'].T[qubits]\n",
    "Bz = data['fields'].T[2*qubits]\n",
    "points = []\n",
    "print(len(Bx))\n",
    "for i in error_pts:\n",
    "    points.append((Bx[i],Bz[i]))\n",
    "print(points)\n",
    "# print(pts2)   \n",
    "# pts3 = expand_pool(pts2, error_pts, 1600)\n",
    "# print(pts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "672a6281-ea10-49f1-97ce-050c556eeb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sum of input lengths does not equal the length of the input dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-245d4aa0d08c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtraining_data_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauto_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset_active\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtraining_data_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtraining_data_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtraining_data_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jszacha1/.conda/envs/conda_env/lib/python3.9/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;31m# Cannot verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sum of input lengths does not equal the length of the input dataset!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sum of input lengths does not equal the length of the input dataset!"
     ]
    }
   ],
   "source": [
    "print(len(pts3))\n",
    "training_data_2 = auto_encoder.get_dataset_active(data_2,2,1600, pts3)\n",
    "training_data_4 = auto_encoder.get_dataset_active(data_4,4,1600, pts3)\n",
    "training_data_7 = auto_encoder.get_dataset_active(data_7,7,1600, pts3) \n",
    "\n",
    "training_data_2, val_data_2 = random_split(training_data_2, [110,10])\n",
    "training_data_4, val_data_4 = random_split(training_data_4, [110,10])\n",
    "training_data_7, val_data_7 = random_split(training_data_7, [110,10])\n",
    "\n",
    "datasets = [training_data_2,\n",
    "            training_data_4,\n",
    "            training_data_7]\n",
    "\n",
    "training_loaders = [DataLoader(x, batch_size = 32,  shuffle=True, num_workers=20) for x in datasets]\n",
    "\n",
    "val_datasets = [val_data_6, val_data_8, val_data_9, val_data_2, val_data_4, val_data_7]\n",
    "\n",
    "val_loaders = [DataLoader(x, batch_size = 10, num_workers=20) for x in val_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b505d69-4918-4cc9-9127-3cb91d1287b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "t = [[0]*3 for i in range(5)]\n",
    "t[0][1]=1\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a95903-3f86-4ff2-aaba-545c629e2ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
