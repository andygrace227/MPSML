One way to structure the network for learning physics is to generalize the
equations of backpropagation beyond homogeneous neurons. This is necessary in
the setting where each of the output neurons is responsible for some portion of
a physical variable, whose domain does not necessarily lie on any specific
portion of the number line. In the case of the ground state of the Schr\"{o}dinger 
equation, one neuron will represent the energy, and the other will track the 
wavefunction coefficients.

There are four fundamental equations of backpropagation. The first describes
the error in each output neuron, effectively how wrong that neuron is. If, in
general, we have a neuron dependent firing mechanism in the output layer, as
well as a neuron dependent cost function, we should write the error as 
\begin{equation}
	\delta^L_j 
	\equiv 
	\frac{\partial C}{\partial a^L_j} 
	\frac{\partial  }{\partial z^L_j}
	f^L_j(z^L_j)
\end{equation}
where we use $L$ to indicate this expression is the error only in the output
layer.  $C$ is the cost function, $a^L_j$ is the activation of the $i^{th}$
neuron, $f^L_j$ is the activation function of the $i^{th}$ neuron and $z^L_j$
is the input to the $i^{th}$ neuron defined by the weighted sum 
$z^l_j = \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j$.

The next expression we need is the expression for the error of any neuron in
layers that are not the output layer. This is defined as 
\begin{equation}
	\delta^l_j
	\equiv
	\sum_k w^{l+1}_{kj} \delta^{l+1}_k
	\frac{\partial}{\partial z^l_j}
	f^l_j(z^l_j)
\end{equation}
where $w^{l+1}_{kj}$ is the synaptic weight matrix connecting the $i$ neurons
in the $(l+1)^{th}$ layer to the $k$ neurons in the $l^{th}$ layer. The
previous two expressions allow the utility of the following two, the first
being 
\begin{equation}
	\frac{\partial C}{\partial b^l_j} \equiv \delta^l_j,
\end{equation}
and the second being
\begin{equation}
	\frac{\partial C}{\partial w^{l}_{jk}} \equiv a^{l-1}_k \delta^l_j,
\end{equation}
to minimize the cost function by backpropagation of the error.

Now, since we ultimately want to learn the ground state of a quantum system, we
will need to have output neurons that fire in specific ways. The ground state
of a quantum system has two important quantities, the energy of the state, and
the wavefunction. The energy lives on the entire real number line, while the
wavefunction is an array of coefficients that live on the interval $[-1,1]$. This 
will be important when we define the activation function for the output neurons 
if they are to represent the ground state of a quantum system.

In order to evaluate these expressions using the backpropagation algorithm, 
we need to define the constraint imposed by the Schr\"{o}dinger equation. 
If we believe that the system under question follows the Schr\"{o}dinger equation, 
then the secular equations impose constraints on the values the output neurons 
can take even just based on the inputs, and the cost of a neuron violating this 
condition can be defined as
\begin{equation}
	C_2 =
	\lambda 
	\Big[ 
		\sum_m \big[\sum_n H_{mn} c_n - E_g c_m \big]^2
	\Big].
\end{equation}
$C_2$ will compose the physical portion of the cost function, but we will retain a 
portion of the cost function associated with how wrong the output neurons are based 
on the data they are exposed to. For the canonical cost function we chose 
the quadratic cost, a fairly basic choice in the machine learning 
community, and perhaps a better choice is available. In any case it is defined as
\begin{equation}
	C_1 = \frac{1}{2} \sum_j (y_j - a_j)^2.
\end{equation}
Our full cost function is then $C = C_1 + C_2$. From these expressions we can
see that the error in the output layer will now be increased when the machine
does not predict values consistent with the physical model. 

The next task is to define the network topology and neuron properties. For the
sake of a canonical example, let us restrict the network topology to three
layers for now.  As alluded to before, there is something special about the
output neurons if they are to output the quantum ground state. The neurons
representing the coefficients of the wavefunction, living on the interval of
$[-1,1]$, we choose to be activated as the hyperbolic tangent, this function
having the same domain. The energy neuron however, is just a real number, so we
choose a linear activation. Therefore, 
\begin{equation}
	a^L_N = \phi \cdot z^L_N
\end{equation}
and 
\begin{equation}
	a^L_j = \mathrm{tanh}(z^L_j).
\end{equation}

For the hidden layer, we can choose hyperbolic
tangent or sigmoid as canonical choices, and will probably test both. 

Before moving on to calculating the gradient of our new cost function, we will
change the notation slightly so that $C_2$ is written in terms of the output
layer neurons. Without loss of generality, we assign the computation of the
ground state energy to the $N^{th}$ neuron of the output layer, and the rest to
the computation of the coefficients. The dimension of the Hilbert space we
denote by $d$, and flatten the Hamiltonian into a row-major array to be used
at the input layer. 
\begin{equation}
	C_2 =
	\lambda 
	\Big[ 
		\sum^{N-1}_m \big[\sum^{N-1}_n a^1_{m \cdot d + n} a^L_n - a^L_N a^L_m \big]^2
	\Big].
\end{equation}

We are making good progress sculpting out this problem, and now we need to calculate 
the derivatives in the expression for the error at the output layer $\delta^L_j$. Care 
must be taken to recognize that the error is neuron dependent, and depends on the 
derivative of the cost function with respect to that neuron, as well as the derivative of the 
activation function of each neuron. So the expression we need to evaluate is
\begin{widetext}
\begin{equation}
	\delta^L_j =
	\frac{\partial}{\partial a^L_j} 
	\Big(
	\frac{1}{2} \sum_k (y_k - a_k)^2 +
	\lambda \Big[ \sum^{N-1}_m \big[\sum^{N-1}_n a^1_{m \cdot d + n} a^L_n - a^L_N a^L_m \big]^2 \Big]
	\Big)
	\frac{\partial}{\partial z^L_j}
	f^L_j(z^L_j).
\end{equation}
It turns out to be easier to focus on first the $j = N$ component representing the energy 
neuron first, then tackle the others. For the $j = N$ component, the expression is 
\begin{equation}
	\delta^L_N =
	\frac{\partial}{\partial a^L_N} 
	\Big(
	\frac{1}{2} \sum_k (y_k - a_k)^2 +
	\lambda \Big[ \sum^{N-1}_m \big[\sum^{N-1}_n a^1_{m \cdot d + n} a^L_n - a^L_N a^L_m \big]^2 \Big]
	\Big)
	\frac{\partial}{\partial z^L_N}
	\phi \cdot z^L_N.
\end{equation}
The first term in only non-zero when $k = N$, and the last derivative can immediately be evaluated.
The result is
\begin{equation}
	\delta^L_N = \phi 
	\Big(
		a_N - y_N + 2 \lambda a^L_N (1 - N)
		\Big[ 
			\sum^{N-1}_m 
			\big[
				\sum^{N-1}_n a^1_{m \cdot d + n} a^L_n - a^L_N a^L_m 
			\big] 
		\Big]
	\Big).
\end{equation}
\end{widetext}
For the other neurons $j < N$, $\delta^L_j$ has a part coming from the
quadratic cost, a part coming from the physics cost, and a part coming from the
activation. The quadratic cost derivative is the same as for the $j = N$ case,
and the activation function for these neurons is the hyperbolic tangent, but
the physics cost is more formidable. The derivative is only non-zero however in
three cases when $m = n = j$, $m = j, n \ne j$, or $n = j, m \ne j$. We will
break this into three terms based on the physics cost for the cases
respectively. The error is then 
\begin{equation}
	\delta^L_j =
	\frac{\partial}{\partial a^L_j} 
	\Big(
	\frac{1}{2} \sum_k (y_k - a_k)^2 +
	\lambda \sum_i T^{(i)}_j 
	\Big)
	\frac{\partial}{\partial z^L_j}
	f^L_j(z^L_j).
\end{equation}
Lets tackle the case when $m = n = j$ and call this $T^{(1)}_j$. The expression to evaluate is
\begin{equation}
	\frac{\partial}{\partial a^L_j}
	T^{(1)}_j = 
	\frac{\partial}{\partial a^L_j} 
	\big[a^1_{j \cdot d + j} a^L_j - a^L_N a^L_j \big]^2 
\end{equation}
and the result is 
\begin{equation}
	T^{(1)}_j = 
	2 [a^1_{j \cdot d + j} a^L_j - a^L_N a^L_j][a^1_{j \cdot d + j} - a^L_N]
\end{equation}
$T^{(2)}_j$ is the term for the case when $m = j, n \ne j$. The expression to evaluate is 
\begin{equation}
	\frac{\partial}{\partial a^L_j}
	T^{(2)}_j =
	\frac{\partial}{\partial a^L_j} 
	\Big[\sum^{N-1}_{n \ne j} a^1_{j \cdot d + n} a^L_n - a^L_N a^L_j \Big]^2,
\end{equation}
and the result is 
\begin{equation}
	T^{(2)}_j =
	2 a^L_N (2 - N) 
	\Big[ \sum^{N-1}_{n \ne j} a^1_{j \cdot d + n} a^L_n - a^L_N a^L_j \Big]
\end{equation}
The final one is for $T^{(3)}_j$, the case of $n = j, m \ne j$. The expression to evaluate is 
\begin{equation}
	\frac{\partial}{\partial a^L_j}
	T^{(3)}_j =
	\frac{\partial}{\partial a^L_j}
	\sum^{N-1}_{m \ne j} \big[a^1_{m \cdot d + j} a^L_j - a^L_N a^L_m \big]^2,
\end{equation}
and the result is 
\begin{equation}
	T^{(3)}_j = 2 
	\Big[ 
		\sum^{N-1}_{m \ne j} 
		(a^1_{m \cdot d + j} a^L_j - a^L_N a^L_m)
		(a^1_{m \cdot d + j})
	\Big]
\end{equation}
And the final expression for the error for wavefunction neurons is
\begin{equation}
	\delta^L_j =
	\Big(
		a^L_j - y_j +
		\lambda \sum_i T^{(i)}_j 
	\Big)
	\Big( 1 - \mathrm{tanh}^2(z^L_j) \Big), \hspace{2mm} \forall \hspace{1mm} j < N
\end{equation}


