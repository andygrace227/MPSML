The distillation of knowledge from data defines one of the most fundamental
human endeavors -- learning, and when viewed as the means by which actionable
information is extracted from raw, unfiltered data, learning forms the basis of
all intelligence. This fundamental principle has revitalized the science of
learning in recent times, with great advancements in both hard and soft
intelligence. And even though many of today's most challenging problems can be
stated elegantly, consonant solutions are rarely, if ever, available. The
consequence of this situation is that finding the best solution is almost
always reduced to an optimization problem, where learning is maximized over
computation. 

Formulation as a minization problem, being unavoidable, affixes serious
challenges to computation of expectations, namely, updating proceeds by
estimation, with uncertainty inversely proportional to effort. In a world
without infinite computational power, one must decide either to have fewer,
accurate updates, or many, poorer updates.  Computational intelligence, also
known as machine learning techniques, have found success in tackling the
optimization problem by mimicking biological systems, but still ultimately
suffer from this fundamental complication.  An exponentially or
combinatorically exploding state space, in many ways, remains the final
resolute barrier that still compels human mediations.

This work posits a mechanism to enhance the capability of soft intelligence
machines' performance in this domain with the addition of an intelligence
prior.  In this approach, the validity of expectations are gauged with respect
not only to the targets, but also required to be consistent with previously
accumulated intelligence. We show that this extension pushes soft computational
machines' extrapolative power beyond the feature space to which they were
exposed, simply because each update is constrained to consistency not only with
the targets, but also with a more general behavioural model.

As proof of concept, we apply this approach to identify quantum phase
transitions in rings of qubits, as well as localization transitions in
disordered, electron-lattice systems. The transverse field Ising chain is a
quintessential physical model, thus is goverened by the Schr\"{o}dinger equation;
maintains an exponentially exploding state space, and has an analytical
solution.  As such, it is a perfect system from which to extract the viability
of this technique. Similarly, localization transitions manifest in the simplest
of electron models, with the governing parameters being well studied
analytically and numerically, and recent applications being suggested in hard
computational intelligence, closing an elegant loop between the two fields.


Ultimately, we demonstrate that the ability of the most primitive of soft
intelligence machines, the fully connected feedforward neural network, to
predict quantum phase transitions can be drastically improved with the simple
addition of a physics prior. The mean square error on the testing datasets
drops much quicker in the physics guided models than in the black box
counterparts, and the predictive power is greatly enhanced. In detecting a
quantum phase transition, the physics guided machine can predict qualitative
changes in the eigenspectrum that the black box machine entirely overlooks.
While identifying a phase transition has been done before with black box
models, these models were exposed to training instances in both phases. The
guided approach codifies an approach that can predict a phase transition having
never been explicitly exposed to it. Therefore, the evolution of physical
observables like the magnetization and the fidelity susceptibility accross the
transition, when sought by two networks of identical topology, can only be
realized with learning guided by an intelligence prior.  This result has deep
implications for the field of computational intelligence, because it provides a
route to augment learning in such a way that predictive power is maximized. 

