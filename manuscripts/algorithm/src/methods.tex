Because learning the energy whilst having the actual data, combined with the
issues of outputs on various domains, does not work and is wasteful with
information we already do have, we have decided to formulate the physics loss
portion of the cost function in a way that is more transparent, and applicable
to a wider range of problems.  The condition we were using before is that the
coefficients of the ground state wavefunction should satisfy the
Schr\"{o}dinger equation.  However, the entire spectrum can be constrained in
the exact same way.  The following of course will hold for any $\lambda_i$ if
every $\vec{\psi_i}$ is an eigenstate. 
\begin{equation}
	\sum_i \lambda_i (\hat{H} \vec{\psi_i} - E_i \vec{\psi_i}) = \vec{0}
\end{equation}
This means we can formulate a matrix representation for the scalar 
physics cost with the help of two auxiliary definitions for an 
energy matrix and a Lagrange matrix. The matrix formulation of the 
equations is important for achieving vectorized operations at the 
implementation phase. If we define the energy matrix as
\begin{equation}
	\hat{E} = \hat{I} \odot (\vec{1}^T \otimes \vec{E}),
\end{equation}
where $\vec{E}$ is a column vector of the eigenvalues. Likewise 
we can define for the Lagrange matrix 
\begin{equation}
	\hat{\lambda} = \hat{I} \odot (\vec{1}^T \otimes \vec{\lambda}).
\end{equation}
We always stick with Hessian notation, so that the primary vectorial 
objects are column vectors and matrices are collections of column 
vectors. In this sense $\vec{1}$ is a column vector of ones and $\vec{0}$
is a column vector of zeros. 

Using these definitions, the scalar physics cost function can be defined as
\begin{equation}
	C_2 = \frac{1}{2} || \hat{H} \hat{\Psi} - \hat{\Psi}\hat{E} ||^2_F.
\end{equation}
The backpropagation equations require the derivative of the cost function, so here we 
can take the matrix derivative 
\begin{equation}
	\frac{\partial C_2}{ \partial \hat{\Psi}} = 
	\hat{H}^2 \hat{\Psi} - 2 \hat{H} \hat{\Psi} \hat{E} + \hat{E}^2 \hat{\Psi}
\end{equation}
In this way, the elements of the result correspond to the gradient of the
physics portion of the cost. This can be much more straight forwardly
implemented with faster vectorized operations. In the matrix implementation of
backpropagation, this expression is evaluated for each instance in the mini
batch.

Finally, the method that has shown the most success has been a direct modulation of the error 
at the output layer.

\begin{equation}
	\hat{\delta}^L += \lambda ( \hat{H} \hat{\Psi} - \hat{\Psi} \hat{E} ) 
\end{equation}
